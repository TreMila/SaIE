{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_details(backward_details):\n",
    "    merged_list = []\n",
    "    current_item = \"\"\n",
    "\n",
    "    for i, item in enumerate(backward_details):\n",
    "        if item.strip() and item.strip()[0].isdigit():\n",
    "            j = 1\n",
    "            while j < len(item.strip()) and item.strip()[j].isdigit():\n",
    "                j += 1\n",
    "            if j < len(item.strip()) and item.strip()[j] == \"：\":\n",
    "                merged_list.append(current_item)\n",
    "                current_item = item.strip()\n",
    "            else:\n",
    "                current_item += item.strip()\n",
    "        else:\n",
    "            current_item += item.strip()\n",
    "\n",
    "    merged_list.append(current_item)\n",
    "    merged_list.pop(0)\n",
    "    return merged_list\n",
    "\n",
    "\n",
    "def extract_tuples(string):\n",
    "    tuples = []\n",
    "    depth = 0\n",
    "    start = None\n",
    "    for i, char in enumerate(string):\n",
    "        if char == \"[\" or char == '(' or char == '（':\n",
    "            if depth == 0:\n",
    "                start = i\n",
    "            depth += 1\n",
    "        elif char == \"]\" or char == ')' or char == '）':\n",
    "            depth -= 1\n",
    "            if depth == 0 and start is not None:\n",
    "                tuples.append(string[start+1:i])\n",
    "                start = None\n",
    "    return tuples\n",
    "\n",
    "\n",
    "def get_results(merged_list, input_list, entity_type_list, eve_extend_map, mode):\n",
    "    input_list2ent = {}\n",
    "    for idx, item in enumerate(input_list):\n",
    "        input_list2ent[item] = entity_type_list[idx//10]\n",
    "\n",
    "    eve_extend_index = 0\n",
    "    cur_eve_extend_list = []\n",
    "    A = []\n",
    "\n",
    "    for idx, item in enumerate(merged_list):\n",
    "        B = []\n",
    "        cur_output_3 = item.split('Output:')\n",
    "        cur_idx = cur_output_3[0].split('：')[0]\n",
    "        cur_example = input_list[int(cur_idx)-1]\n",
    "       \n",
    "        cur_true_eve = input_list2ent[cur_example]\n",
    "\n",
    "        if eve_extend_index == len(cur_eve_extend_list):\n",
    "            eve_extend_index = 0\n",
    "\n",
    "        cur_eve_extend_list = eve_extend_map[cur_true_eve]\n",
    "        cur_extend_eve = cur_eve_extend_list[eve_extend_index]\n",
    "\n",
    "\n",
    "        for output in cur_output_3[1:]:\n",
    "            sep = \"\"\n",
    "            if 'zh' in mode:\n",
    "                output = output.replace(\":\", \"：\")\n",
    "                if \"生成事件检测列表:\" in output:\n",
    "                    sep = \"生成事件检测列表:\"\n",
    "                elif \"生成事件检测列表：\" in output:\n",
    "                    sep = \"生成事件检测列表：\"\n",
    "            elif 'en' in mode:\n",
    "                if \"Generate a list of event detections:\" in output:\n",
    "                    sep = \"Generate a list of event detections:\"\n",
    "                elif \"Generated event detection list:\" in output:\n",
    "                    sep = \"Generated event detection list:\"\n",
    "                elif \"Event detection list:\" in output:\n",
    "                    sep = \"Event detection list:\"\n",
    "                elif \"Event detections:\" in output:\n",
    "                    sep = \"Event detections:\"\n",
    "                elif \"event detection list:\" in output:\n",
    "                    sep = \"event detection list:\"\n",
    "                elif \"List of Event Detections:\" in output:\n",
    "                    sep = \"List of Event Detections:\"\n",
    "                elif \"generate a list of event detections:\" in output:\n",
    "                    sep = \"generate a list of event detections:\"\n",
    "                elif \"generate event detection list:\" in output:\n",
    "                    sep = \"generate event detection list:\"\n",
    "                elif \"event detection list:\" in output:\n",
    "                    sep = \"event detection list:\"\n",
    "                # elif \"Trigger span:\" in output:\n",
    "                #     sep = \"Trigger span:\"\n",
    "                # elif \"Trigger word span:\" in output:\n",
    "                #     sep = \"Trigger word span:\"\n",
    "                # elif \"Trigger Word:\" in output:\n",
    "                #     sep = \"Trigger Word:\"\n",
    "                # elif \"The trigger word for this case is \" in output:\n",
    "                #     sep = \"The trigger word for this case is \"\n",
    "                # elif \"The trigger word for this event type is \" in output:\n",
    "                #     sep = \"The trigger word for this event type is \"\n",
    "            if sep:\n",
    "                tmp = output.split(sep)[1]\n",
    "                matches = extract_tuples(tmp)\n",
    "                if matches:\n",
    "                    for match in matches:\n",
    "                        if 'zh' in mode:\n",
    "                            triple = tuple(match.strip().replace('，',',').strip('（）').split(','))\n",
    "                        elif 'en' in mode:\n",
    "                            triple = tuple(match.strip().strip('()').split(','))\n",
    "                        if len(triple) == 2:\n",
    "                            B.append((int(cur_idx)-1, cur_extend_eve, triple[1].strip()))\n",
    "                        else:\n",
    "                            B.append((int(cur_idx)-1, cur_extend_eve, ''))\n",
    "                else:\n",
    "                    B.append((int(cur_idx)-1, cur_extend_eve, ''))\n",
    "                \n",
    "            else:\n",
    "                B.append((int(cur_idx)-1, cur_extend_eve, ''))\n",
    "                continue\n",
    "        if B:\n",
    "            A.append(B)    \n",
    "\n",
    "        eve_extend_index += 1\n",
    "\n",
    "    A = [list(set(item)) for item in A]\n",
    "    \n",
    "    with open(f'./results/backward_{mode}.txt','w',encoding='utf-8') as f:\n",
    "        for item in A:\n",
    "            f.write(str(item)+'\\n')\n",
    "\n",
    "\n",
    "def get_gold_results(merged_list, input_list, entity_type_list, mode):\n",
    "    input_list2ent = {}\n",
    "    for idx, item in enumerate(input_list):\n",
    "        input_list2ent[item] = entity_type_list[idx//10]\n",
    "\n",
    "    A = []\n",
    "\n",
    "    for idx, item in enumerate(merged_list):\n",
    "        B = []\n",
    "        cur_output_3 = item.split('Output:')\n",
    "        cur_idx = cur_output_3[0].split('：')[0]\n",
    "        cur_example = input_list[int(cur_idx)-1]\n",
    "        cur_true_eve = input_list2ent[cur_example]\n",
    "\n",
    "\n",
    "        for output in cur_output_3[1:]:\n",
    "\n",
    "            sep = \"\"\n",
    "            if 'zh' in mode:\n",
    "                output = output.replace(\":\", \"：\")\n",
    "                if \"生成事件检测列表:\" in output:\n",
    "                    sep = \"生成事件检测列表:\"\n",
    "                elif \"生成事件检测列表：\" in output:\n",
    "                    sep = \"生成事件检测列表：\"\n",
    "            elif 'en' in mode:\n",
    "                if \"Generate a list of event detections:\" in output:\n",
    "                    sep = \"Generate a list of event detections:\"\n",
    "                elif \"Generated event detection list:\" in output:\n",
    "                    sep = \"Generated event detection list:\"\n",
    "                elif \"Event detection list:\" in output:\n",
    "                    sep = \"Event detection list:\"\n",
    "                elif \"Event detections:\" in output:\n",
    "                    sep = \"Event detections:\"\n",
    "                elif \"event detection list:\" in output:\n",
    "                    sep = \"event detection list:\"\n",
    "                elif \"List of Event Detections:\" in output:\n",
    "                    sep = \"List of Event Detections:\"\n",
    "                elif \"generate a list of event detections:\" in output:\n",
    "                    sep = \"generate a list of event detections:\"\n",
    "                elif \"generate event detection list:\" in output:\n",
    "                    sep = \"generate event detection list:\"\n",
    "                elif \"event detection list:\" in output:\n",
    "                    sep = \"event detection list:\"\n",
    "                # elif \"Trigger span:\" in output:\n",
    "                #     sep = \"Trigger span:\"\n",
    "                # elif \"Trigger word span:\" in output:\n",
    "                #     sep = \"Trigger word span:\"\n",
    "                # elif \"Trigger Word:\" in output:\n",
    "                #     sep = \"Trigger Word:\"\n",
    "                # elif \"The trigger word for this case is \" in output:\n",
    "                #     sep = \"The trigger word for this case is \"\n",
    "                # elif \"The trigger word for this event type is \" in output:\n",
    "                #     sep = \"The trigger word for this event type is \"\n",
    "            if sep:\n",
    "                tmp = output.split(sep)[1]\n",
    "                matches = extract_tuples(tmp)\n",
    "                if matches:\n",
    "                    for match in matches:\n",
    "                        if 'zh' in mode:\n",
    "                            triple = tuple(match.strip().replace('，',',').strip('（）').split(','))\n",
    "                        elif 'en' in mode:\n",
    "                            triple = tuple(match.strip().strip('()').split(','))\n",
    "                        if len(triple) == 2:\n",
    "                            B.append((int(cur_idx)-1, cur_true_eve, triple[1].strip()))\n",
    "                        else:\n",
    "                            B.append((int(cur_idx)-1, cur_true_eve, ''))\n",
    "                else:\n",
    "                    B.append((int(cur_idx)-1, cur_true_eve, ''))\n",
    "                \n",
    "            else:\n",
    "                B.append((int(cur_idx)-1, cur_true_eve, ''))\n",
    "                continue\n",
    "        if B:\n",
    "            A.append(B)    \n",
    "\n",
    "    A = [list(set(item)) for item in A]\n",
    "    \n",
    "    with open(f'./results/backward_gold_{mode}.txt','w',encoding='utf-8') as f:\n",
    "        for item in A:\n",
    "            f.write(str(item)+'\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DuEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results/details_zh.txt','r',encoding='utf-8') as f:\n",
    "    backward_details = f.readlines()\n",
    "\n",
    "with open('./results/details_gold_zh.txt','r',encoding='utf-8') as f:\n",
    "    backward_gold_details = f.readlines()\n",
    "\n",
    "with open('./data/duee/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/duee/final_event_extend_map_zh.json','r',encoding='utf-8') as f:\n",
    "    eve_extend_map = json.load(f)\n",
    "\n",
    "with open('../DuEE/processed_data/labels.txt','r',encoding='utf-8') as f:\n",
    "    event_type_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "merged_list = merge_details(backward_details)\n",
    "get_results(merged_list, input_list, event_type_list, eve_extend_map, 'zh')\n",
    "\n",
    "merged_list = merge_details(backward_gold_details)\n",
    "get_gold_results(merged_list, input_list, event_type_list, 'zh')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CASIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results/details_en.txt','r',encoding='utf-8') as f:\n",
    "    backward_details = f.readlines()\n",
    "\n",
    "with open('./results/details_gold_en.txt','r',encoding='utf-8') as f:\n",
    "    backward_gold_details = f.readlines()\n",
    "\n",
    "with open('./data/casie/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/casie/final_event_extend_map_en.json','r',encoding='utf-8') as f:\n",
    "    eve_extend_map = json.load(f)\n",
    "\n",
    "with open('../CASIE/processed_data/labels.txt','r',encoding='utf-8') as f:\n",
    "    event_type_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "merged_list = merge_details(backward_details)\n",
    "get_results(merged_list, input_list, event_type_list, eve_extend_map, 'en')\n",
    "\n",
    "merged_list = merge_details(backward_gold_details)\n",
    "get_gold_results(merged_list, input_list, event_type_list, 'en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpaca-DuEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results/details_alpaca_33B_zh.txt','r',encoding='utf-8') as f:\n",
    "    backward_details = f.readlines()\n",
    "\n",
    "with open('./results/details_gold_alpaca_33B_zh.txt','r',encoding='utf-8') as f:\n",
    "    backward_gold_details = f.readlines()\n",
    "\n",
    "with open('./data/duee/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/duee/final_event_extend_map_alpaca_33B_zh.json','r',encoding='utf-8') as f:\n",
    "    eve_extend_map = json.load(f)\n",
    "\n",
    "with open('../DuEE/processed_data/labels.txt','r',encoding='utf-8') as f:\n",
    "    event_type_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "merged_list = merge_details(backward_details)\n",
    "get_results(merged_list, input_list, event_type_list, eve_extend_map, 'alpaca_33B_zh')\n",
    "\n",
    "merged_list = merge_details(backward_gold_details)\n",
    "get_gold_results(merged_list, input_list, event_type_list, 'alpaca_33B_zh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpaca-CASIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results/details_alpaca_33B_en.txt','r',encoding='utf-8') as f:\n",
    "    backward_details = f.readlines()\n",
    "\n",
    "with open('./results/details_gold_alpaca_33B_en.txt','r',encoding='utf-8') as f:\n",
    "    backward_gold_details = f.readlines()\n",
    "\n",
    "with open('./data/casie/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/casie/final_event_extend_map_alpaca_33B_en.json','r',encoding='utf-8') as f:\n",
    "    eve_extend_map = json.load(f)\n",
    "\n",
    "with open('../CASIE/processed_data/labels.txt','r',encoding='utf-8') as f:\n",
    "    event_type_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "merged_list = merge_details(backward_details)\n",
    "get_results(merged_list, input_list, event_type_list, eve_extend_map, 'alpaca_33B_en')\n",
    "\n",
    "merged_list = merge_details(backward_gold_details)\n",
    "get_gold_results(merged_list, input_list, event_type_list, 'alpaca_33B_en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llama2-CASIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results/details_llama2_70B_en.txt','r',encoding='utf-8') as f:\n",
    "    backward_details = f.readlines()\n",
    "\n",
    "with open('./results/details_gold_llama2_70B_en.txt','r',encoding='utf-8') as f:\n",
    "    backward_gold_details = f.readlines()\n",
    "\n",
    "with open('./data/casie/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/casie/final_event_extend_map_llama2_70B_en.json','r',encoding='utf-8') as f:\n",
    "    eve_extend_map = json.load(f)\n",
    "\n",
    "with open('../CASIE/processed_data/labels.txt','r',encoding='utf-8') as f:\n",
    "    event_type_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "merged_list = merge_details(backward_details)\n",
    "get_results(merged_list, input_list, event_type_list, eve_extend_map, 'llama2_70B_en')\n",
    "\n",
    "merged_list = merge_details(backward_gold_details)\n",
    "get_gold_results(merged_list, input_list, event_type_list,'llama2_70B_en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGLM-DuEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results/details_chatglm_6B_zh.txt','r',encoding='utf-8') as f:\n",
    "    backward_details = f.readlines()\n",
    "\n",
    "with open('./results/details_gold_chatglm_6B_zh.txt','r',encoding='utf-8') as f:\n",
    "    backward_gold_details = f.readlines()\n",
    "\n",
    "with open('./data/duee/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/duee/final_event_extend_map_chatglm_6B_zh.json','r',encoding='utf-8') as f:\n",
    "    eve_extend_map = json.load(f)\n",
    "\n",
    "with open('../DuEE/processed_data/labels.txt','r',encoding='utf-8') as f:\n",
    "    event_type_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "merged_list = merge_details(backward_details)\n",
    "get_results(merged_list, input_list, event_type_list, eve_extend_map, 'chatglm_6B_zh')\n",
    "\n",
    "merged_list = merge_details(backward_gold_details)\n",
    "get_gold_results(merged_list, input_list, event_type_list, 'chatglm_6B_zh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGLM-CASIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results/details_chatglm_6B_en.txt','r',encoding='utf-8') as f:\n",
    "    backward_details = f.readlines()\n",
    "\n",
    "with open('./results/details_gold_chatglm_6B_en.txt','r',encoding='utf-8') as f:\n",
    "    backward_gold_details = f.readlines()\n",
    "\n",
    "with open('./data/casie/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/casie/final_event_extend_map_chatglm_6B_en.json','r',encoding='utf-8') as f:\n",
    "    eve_extend_map = json.load(f)\n",
    "\n",
    "with open('../CASIE/processed_data/labels.txt','r',encoding='utf-8') as f:\n",
    "    event_type_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "merged_list = merge_details(backward_details)\n",
    "get_results(merged_list, input_list, event_type_list, eve_extend_map, 'chatglm_6B_en')\n",
    "\n",
    "merged_list = merge_details(backward_gold_details)\n",
    "get_gold_results(merged_list, input_list, event_type_list, 'chatglm_6B_en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT4-DuEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results/details_gpt4_zh.txt','r',encoding='utf-8') as f:\n",
    "    backward_details = f.readlines()\n",
    "\n",
    "with open('./data/duee/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/duee/final_event_extend_map_gpt4_zh.json','r',encoding='utf-8') as f:\n",
    "    eve_extend_map = json.load(f)\n",
    "\n",
    "with open('../DuEE/processed_data/labels.txt','r',encoding='utf-8') as f:\n",
    "    event_type_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "merged_list = merge_details(backward_details)\n",
    "get_results(merged_list, input_list, event_type_list, eve_extend_map, 'gpt4_zh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT4-CASIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results/details_gpt4_en.txt','r',encoding='utf-8') as f:\n",
    "    backward_details = f.readlines()\n",
    "\n",
    "with open('./data/casie/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/casie/final_event_extend_map_gpt4_en.json','r',encoding='utf-8') as f:\n",
    "    eve_extend_map = json.load(f)\n",
    "\n",
    "with open('../CASIE/processed_data/labels.txt','r',encoding='utf-8') as f:\n",
    "    event_type_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "merged_list = merge_details(backward_details)\n",
    "get_results(merged_list, input_list, event_type_list, eve_extend_map, 'gpt4_en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baichuan2-DuEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results/details_baichuan2_13B_zh.txt','r',encoding='utf-8') as f:\n",
    "    backward_details = f.readlines()\n",
    "\n",
    "with open('./results/details_gold_baichuan2_13B_zh.txt','r',encoding='utf-8') as f:\n",
    "    backward_gold_details = f.readlines()\n",
    "\n",
    "with open('./data/duee/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/duee/final_event_extend_map_baichuan2_13B_zh.json','r',encoding='utf-8') as f:\n",
    "    eve_extend_map = json.load(f)\n",
    "\n",
    "with open('../DuEE/processed_data/labels.txt','r',encoding='utf-8') as f:\n",
    "    event_type_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "merged_list = merge_details(backward_details)\n",
    "get_results(merged_list, input_list, event_type_list, eve_extend_map, 'baichuan2_13B_zh')\n",
    "\n",
    "merged_list = merge_details(backward_gold_details)\n",
    "get_gold_results(merged_list, input_list, event_type_list, 'baichuan2_13B_zh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baichuan2-CASIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results/details_baichuan2_13B_en.txt','r',encoding='utf-8') as f:\n",
    "    backward_details = f.readlines()\n",
    "\n",
    "with open('./results/details_gold_baichuan2_13B_en.txt','r',encoding='utf-8') as f:\n",
    "    backward_gold_details = f.readlines()\n",
    "\n",
    "with open('./data/casie/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/casie/final_event_extend_map_baichuan2_13B_en.json','r',encoding='utf-8') as f:\n",
    "    eve_extend_map = json.load(f)\n",
    "\n",
    "with open('../CASIE/processed_data/labels.txt','r',encoding='utf-8') as f:\n",
    "    event_type_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "merged_list = merge_details(backward_details)\n",
    "get_results(merged_list, input_list, event_type_list, eve_extend_map, 'baichuan2_13B_en')\n",
    "\n",
    "merged_list = merge_details(backward_gold_details)\n",
    "get_gold_results(merged_list, input_list, event_type_list, 'baichuan2_13B_en')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
