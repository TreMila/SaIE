{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T11:00:40.458753Z",
     "start_time": "2023-12-11T11:00:40.452651Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T11:00:40.887127Z",
     "start_time": "2023-12-11T11:00:40.870041Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_type(extend_map, label):\n",
    "    for key, val in extend_map.items():\n",
    "        for v in val:\n",
    "            if v == label:\n",
    "                return key\n",
    "\n",
    "\n",
    "def get_count_matrix(rel_extend_map, merged_golds, test):\n",
    "    new_rel_extend_map = {k:[] for k in rel_extend_map.keys()}\n",
    "    for k in new_rel_extend_map.keys():\n",
    "#         new_rel_extend_map[k] = [k] + rel_extend_map[k]\n",
    "        new_rel_extend_map[k] = rel_extend_map[k]\n",
    "    value = [c for v in new_rel_extend_map.values() for c in v]\n",
    "    dict = {k: [] for k in value}\n",
    "    dict_count = {k: [] for k in value}\n",
    "\n",
    "    for item in test:\n",
    "        lab = item[0][1]\n",
    "        dict[lab].append(item)\n",
    "\n",
    "    for k,v in dict.items():   \n",
    "        cur_extend_rel = k\n",
    "        cur_list = v\n",
    "        for item in cur_list:\n",
    "            count_correct = 0\n",
    "            count_wrong_from_gold = 0\n",
    "            count_wrong_from_pred = 0\n",
    "\n",
    "            cur_gold_rel = get_type(new_rel_extend_map, cur_extend_rel)\n",
    "            cur_idx = item[0][0]\n",
    "            cur_merged_golds = merged_golds[cur_idx]\n",
    "            cur_golds = [gold for gold in cur_merged_golds if gold[1] == cur_gold_rel]\n",
    "\n",
    "            cur_preds = [pred for pred in item] \n",
    "\n",
    "            preds_length = len(cur_preds)\n",
    "            golds_length = len(cur_golds)\n",
    "\n",
    "            if golds_length != 0:\n",
    "                for pred in cur_preds:\n",
    "                    for cmp_gold in cur_golds:\n",
    "                        if pred[2] == '' and pred[3] == '':\n",
    "                            preds_length -= 1\n",
    "                            break\n",
    "                        elif pred[2] == '' or pred[3] == '':\n",
    "                            continue\n",
    "                        elif (pred[2] in cmp_gold[2] and pred[3] in cmp_gold[3]) \\\n",
    "                            or (cmp_gold[2] in pred[2] and cmp_gold[3] in pred[3]) \\\n",
    "                            or (pred[2] in cmp_gold[2] and cmp_gold[3] in pred[3]) \\\n",
    "                            or (cmp_gold[2] in pred[2] and pred[3] in cmp_gold[3]):\n",
    "                            count_correct += 1\n",
    "                            break\n",
    "            count_correct = min(count_correct, golds_length)\n",
    "            count_wrong_from_gold = golds_length - count_correct\n",
    "            count_wrong_from_pred = preds_length - count_correct\n",
    "            dict_count[cur_extend_rel].append((count_correct, count_wrong_from_gold, count_wrong_from_pred))\n",
    "\n",
    "    matrix = [v for _,v in dict_count.items()]\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T11:00:41.725800Z",
     "start_time": "2023-12-11T11:00:41.713795Z"
    }
   },
   "outputs": [],
   "source": [
    "def word_level(rel_extend_map, matrix):\n",
    "    new_rel_extend_map = {k:[] for k in rel_extend_map.keys()}\n",
    "    for k in new_rel_extend_map.keys():\n",
    "#         new_rel_extend_map[k] = [k] + rel_extend_map[k]\n",
    "        new_rel_extend_map[k] = rel_extend_map[k]\n",
    "    value = [c for v in new_rel_extend_map.values() for c in v]\n",
    "    word_count_dict = {k:[] for k in value}\n",
    "\n",
    "    tmp_list = []\n",
    "\n",
    "    for idx, row in enumerate(matrix):\n",
    "        correct_sum = 0\n",
    "        wrong_from_gold_sum = 0\n",
    "        wrong_from_pred_sum = 0\n",
    "        for tuple in row:\n",
    "            correct_sum += tuple[0]\n",
    "            wrong_from_gold_sum += tuple[1]\n",
    "            wrong_from_pred_sum += tuple[2]\n",
    "        if correct_sum + wrong_from_pred_sum == 0:\n",
    "            P = 0.0\n",
    "        else:\n",
    "            P = correct_sum / (correct_sum + wrong_from_pred_sum)\n",
    "        if correct_sum + wrong_from_gold_sum == 0:\n",
    "            R = 0.0\n",
    "        else:\n",
    "            R = correct_sum / (correct_sum + wrong_from_gold_sum)\n",
    "        \n",
    "        if P + R == 0:\n",
    "            F1 = 0.0\n",
    "        else:\n",
    "            F1 = 2 * P * R / (P + R)\n",
    "        P_1 = 1 - P\n",
    "        R_1 = 1 - R\n",
    "        tmp_list.append((P, R, P_1, R_1, F1, correct_sum, wrong_from_gold_sum, wrong_from_pred_sum))\n",
    "\n",
    "\n",
    "    for idx,k in enumerate(word_count_dict.keys()):\n",
    "        word_count_dict[k] = tmp_list[idx]\n",
    "\n",
    "    return word_count_dict\n",
    "\n",
    "\n",
    "def word_dict2execl(word_count_dict, rel_extend_map, human_sim, mode):\n",
    "    new_rel_extend_map = {k:[] for k in rel_extend_map.keys()}\n",
    "    for k in new_rel_extend_map.keys():\n",
    "        # new_rel_extend_map[k] = [k] + rel_extend_map[k]\n",
    "        new_rel_extend_map[k] = rel_extend_map[k]\n",
    "\n",
    "    df = pd.DataFrame.from_dict(word_count_dict, orient='index').reset_index()\n",
    "    df.columns = ['扩展关系词','P','R','1-P','1-R','F1','正确个数','golds中错误个数','preds中错误个数']\n",
    "    extend_rel_list = df['扩展关系词'].tolist()\n",
    "\n",
    "    gold_rel_list = [get_type(new_rel_extend_map, rel) for rel in extend_rel_list]\n",
    "    df['关系类型'] = gold_rel_list\n",
    "\n",
    "    cols = list(df.columns)\n",
    "    cols.insert(0, cols.pop(cols.index('关系类型')))\n",
    "    df = df.loc[:, cols]\n",
    "    df.insert(loc=2, column='是否语义相关', value=human_sim)\n",
    "\n",
    "    df.to_excel(f'./results/word_count_dict_{mode}.xlsx',index=False)\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T11:00:42.470506Z",
     "start_time": "2023-12-11T11:00:42.457237Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def split_sheet(groupby_col, mode):  \n",
    "    df = pd.read_excel(f'./results/word_count_dict_{mode}.xlsx')\n",
    "    df_grouped = df.groupby(groupby_col)\n",
    "    type_list = list(df[groupby_col].unique())\n",
    "\n",
    "    with pd.ExcelWriter(f'./results/word_count_dict_{mode}_中间处理表.xlsx', engine='xlsxwriter') as writer:\n",
    "        for type in type_list:\n",
    "            sub_df = df_grouped.get_group(type).copy().reset_index(drop=True)\n",
    "            if r'/' in type:\n",
    "                type = type.replace(r'/', '')\n",
    "            sub_df.to_excel(writer, sheet_name=type, index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CMeIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/cmeie/rel_extend_map.json','r',encoding='utf-8') as f:\n",
    "    rel_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/cmeie/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "\n",
    "with open('./results/test_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    test = [eval(line) for line in f]\n",
    "\n",
    "matrix = get_count_matrix(rel_extend_map, merged_golds, test)\n",
    "word_count_dict = word_level(rel_extend_map, matrix)\n",
    "df_word = word_dict2execl(word_count_dict, rel_extend_map, mode='zh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCIERC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/scierc/rel_extend_map.json','r',encoding='utf-8') as f:\n",
    "    rel_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/scierc/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "\n",
    "with open('./results/test_en.txt', 'r', encoding='utf-8') as f:\n",
    "    test = [eval(line) for line in f]\n",
    "\n",
    "\n",
    "matrix = get_count_matrix(rel_extend_map, merged_golds, test)\n",
    "word_count_dict = word_level(rel_extend_map, matrix)\n",
    "df_word = word_dict2execl(word_count_dict, rel_extend_map, mode='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baichuan2-CMeIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T11:00:46.917519Z",
     "start_time": "2023-12-11T11:00:46.658412Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('./data/cmeie/final_rel_extend_map_baichuan2_13B_zh.json','r',encoding='utf-8') as f:\n",
    "    rel_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/cmeie/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "\n",
    "with open('./results/test_baichuan2_13B_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    test = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/cmeie/rel_sim_human_baichuan2_13B_zh.json','r',encoding='utf-8') as f:\n",
    "    sim_en = json.load(f)\n",
    "res_dict = {k:{} for k in rel_extend_map.keys()}\n",
    "for key in rel_extend_map.keys():\n",
    "    for value in sim_en[key].keys():\n",
    "        if value in rel_extend_map[key]:\n",
    "            res_dict[key][value] = sim_en[key][value]\n",
    "\n",
    "with open('./data/cmeie/final_rel_sim_human_baichuan2_13B_zh.json','w',encoding='utf-8') as f:\n",
    "    json.dump(res_dict,f,indent=4,ensure_ascii=False)\n",
    "\n",
    "human_sim = []\n",
    "for key in res_dict.keys():\n",
    "    for k,v in res_dict[key].items():\n",
    "        human_sim.append(v)\n",
    "        \n",
    "matrix = get_count_matrix(rel_extend_map, merged_golds, test)\n",
    "word_count_dict = word_level(rel_extend_map, matrix)\n",
    "df_word = word_dict2execl(word_count_dict, rel_extend_map, human_sim, mode='baichuan2_13B_zh')\n",
    "\n",
    "split_sheet('关系类型','baichuan2_13B_zh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baichuan2-SCIERC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T11:00:48.730564Z",
     "start_time": "2023-12-11T11:00:48.259999Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('./data/scierc/final_rel_extend_map_baichuan2_13B_en.json','r',encoding='utf-8') as f:\n",
    "    rel_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/scierc/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "\n",
    "with open('./results/test_baichuan2_13B_en.txt', 'r', encoding='utf-8') as f:\n",
    "    test = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/scierc/rel_sim_human_baichuan2_13B_en.json','r',encoding='utf-8') as f:\n",
    "    sim_en = json.load(f)\n",
    "res_dict = {k:{} for k in rel_extend_map.keys()}\n",
    "for key in rel_extend_map.keys():\n",
    "    for value in sim_en[key].keys():\n",
    "        if value in rel_extend_map[key]:\n",
    "            res_dict[key][value] = sim_en[key][value]\n",
    "\n",
    "with open('./data/scierc/final_rel_sim_human_baichuan2_13B_en.json','w',encoding='utf-8') as f:\n",
    "    json.dump(res_dict,f,indent=4,ensure_ascii=False)\n",
    "\n",
    "human_sim = []\n",
    "for key in res_dict.keys():\n",
    "    for k,v in res_dict[key].items():\n",
    "        human_sim.append(v)\n",
    "        \n",
    "matrix = get_count_matrix(rel_extend_map, merged_golds, test)\n",
    "word_count_dict = word_level(rel_extend_map, matrix)\n",
    "df_word = word_dict2execl(word_count_dict, rel_extend_map, human_sim,mode='baichuan2_13B_en')\n",
    "\n",
    "split_sheet('关系类型','baichuan2_13B_en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/cmeie/final_rel_extend_map_alpaca_33B_zh.json','r',encoding='utf-8') as f:\n",
    "    rel_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/cmeie/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "\n",
    "with open('./results/test_alpaca_33B_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    test = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/cmeie/rel_sim_human_alpaca_33B_zh.json','r',encoding='utf-8') as f:\n",
    "    sim_en = json.load(f)\n",
    "res_dict = {k:{} for k in rel_extend_map.keys()}\n",
    "for key in rel_extend_map.keys():\n",
    "    for value in sim_en[key].keys():\n",
    "        if value in rel_extend_map[key]:\n",
    "            res_dict[key][value] = sim_en[key][value]\n",
    "\n",
    "with open('./data/cmeie/final_rel_sim_human_alpaca_33B_zh.json','w',encoding='utf-8') as f:\n",
    "    json.dump(res_dict,f,indent=4,ensure_ascii=False)\n",
    "\n",
    "human_sim = []\n",
    "for key in res_dict.keys():\n",
    "    for k,v in res_dict[key].items():\n",
    "        human_sim.append(v)\n",
    "        \n",
    "matrix = get_count_matrix(rel_extend_map, merged_golds, test)\n",
    "word_count_dict = word_level(rel_extend_map, matrix)\n",
    "df_word = word_dict2execl(word_count_dict, rel_extend_map, human_sim, mode='alpaca_33B_zh')\n",
    "\n",
    "split_sheet('关系类型','alpaca_33B_zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/scierc/final_rel_extend_map_alpaca_33B_en.json','r',encoding='utf-8') as f:\n",
    "    rel_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/scierc/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "\n",
    "with open('./results/test_alpaca_33B_en.txt', 'r', encoding='utf-8') as f:\n",
    "    test = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/scierc/rel_sim_human_alpaca_33B_en.json','r',encoding='utf-8') as f:\n",
    "    sim_en = json.load(f)\n",
    "res_dict = {k:{} for k in rel_extend_map.keys()}\n",
    "for key in rel_extend_map.keys():\n",
    "    for value in sim_en[key].keys():\n",
    "        if value in rel_extend_map[key]:\n",
    "            res_dict[key][value] = sim_en[key][value]\n",
    "\n",
    "with open('./data/scierc/final_rel_sim_human_alpaca_33B_en.json','w',encoding='utf-8') as f:\n",
    "    json.dump(res_dict,f,indent=4,ensure_ascii=False)\n",
    "\n",
    "human_sim = []\n",
    "for key in res_dict.keys():\n",
    "    for k,v in res_dict[key].items():\n",
    "        human_sim.append(v)\n",
    "        \n",
    "matrix = get_count_matrix(rel_extend_map, merged_golds, test)\n",
    "word_count_dict = word_level(rel_extend_map, matrix)\n",
    "df_word = word_dict2execl(word_count_dict, rel_extend_map, human_sim,mode='alpaca_33B_en')\n",
    "\n",
    "split_sheet('关系类型','alpaca_33B_en')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
