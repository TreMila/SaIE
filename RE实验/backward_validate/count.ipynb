{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type(rel_extend_map, rel):\n",
    "    for key, val in rel_extend_map.items():\n",
    "        for v in val:\n",
    "            if v == rel:\n",
    "                return key\n",
    "\n",
    "\n",
    "def get_count_matrix(rel_extend_map, merged_golds, backward):\n",
    "    value = [c for v in rel_extend_map.values() for c in v]\n",
    "    dict = {k: [] for k in value}\n",
    "    dict_count = {k: [] for k in value}\n",
    "\n",
    "    for item in backward:\n",
    "        rel = item[0][1]\n",
    "        dict[rel].append(item)\n",
    "\n",
    "    for k,v in dict.items():   \n",
    "        cur_extend_rel = k\n",
    "        cur_list = v\n",
    "        for item in cur_list:\n",
    "            count_correct = 0\n",
    "            count_wrong_from_gold = 0\n",
    "            count_wrong_from_pred = 0\n",
    "\n",
    "            cur_gold_rel = get_type(rel_extend_map, cur_extend_rel)\n",
    "            cur_idx = item[0][0]\n",
    "\n",
    "            cur_merged_golds = merged_golds[cur_idx]\n",
    "            if type(cur_merged_golds) == tuple:\n",
    "                cur_golds = [cur_merged_golds]\n",
    "            else:\n",
    "                cur_golds = [gold for gold in cur_merged_golds if gold[1] == cur_gold_rel]\n",
    "            cur_preds = [pred for pred in item] \n",
    "\n",
    "            preds_length = len(cur_preds)\n",
    "            golds_length = len(cur_golds)\n",
    "\n",
    "            if golds_length != 0:\n",
    "                for pred in cur_preds:\n",
    "                    for cmp_gold in cur_golds:\n",
    "                        if pred[2] == '' and pred[3] == '':\n",
    "                            preds_length -= 1\n",
    "                            break\n",
    "                        elif pred[2] == '' or pred[3] == '':\n",
    "                            continue\n",
    "                        elif (pred[2] in cmp_gold[2] and pred[3] in cmp_gold[3]) \\\n",
    "                            or (cmp_gold[2] in pred[2] and cmp_gold[3] in pred[3]) \\\n",
    "                            or (pred[2] in cmp_gold[2] and cmp_gold[3] in pred[3]) \\\n",
    "                            or (cmp_gold[2] in pred[2] and pred[3] in cmp_gold[3]):\n",
    "                            count_correct += 1\n",
    "                            break\n",
    "            count_correct = min(count_correct, golds_length)\n",
    "            count_wrong_from_gold = golds_length - count_correct\n",
    "            count_wrong_from_pred = preds_length - count_correct\n",
    "            dict_count[cur_extend_rel].append((count_correct, count_wrong_from_gold, count_wrong_from_pred))\n",
    "       \n",
    "    matrix = [v for _,v in dict_count.items()]\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def get_gold_count_matrix(rel_extend_map, merged_golds, backward):\n",
    "    dict = {k: [] for k in rel_extend_map.keys()}\n",
    "    dict_count = {k: [] for k in rel_extend_map.keys()}\n",
    "\n",
    "    for item in backward:\n",
    "        rel = item[0][1]\n",
    "        dict[rel].append(item)\n",
    "\n",
    "    for k,v in dict.items():   \n",
    "        cur_gold_rel = k\n",
    "        cur_list = v\n",
    "        for item in cur_list:\n",
    "            count_correct = 0\n",
    "            count_wrong_from_gold = 0\n",
    "            count_wrong_from_pred = 0\n",
    "\n",
    "            cur_idx = item[0][0]\n",
    "            cur_merged_golds = merged_golds[cur_idx]\n",
    "            if type(cur_merged_golds) == tuple:\n",
    "                cur_golds = [cur_merged_golds]\n",
    "            else:\n",
    "                cur_golds = [gold for gold in cur_merged_golds if gold[1] == cur_gold_rel]\n",
    "            cur_preds = [pred for pred in item] \n",
    "\n",
    "            preds_length = len(cur_preds)\n",
    "            golds_length = len(cur_golds)\n",
    "\n",
    "            if golds_length != 0:\n",
    "                for pred in cur_preds:\n",
    "                    for cmp_gold in cur_golds:\n",
    "                        if pred[2] == '' and pred[3] == '':\n",
    "                            preds_length -= 1\n",
    "                            break\n",
    "                        elif pred[2] == '' or pred[3] == '':\n",
    "                            continue\n",
    "                        elif (pred[2] in cmp_gold[2] and pred[3] in cmp_gold[3]) \\\n",
    "                            or (cmp_gold[2] in pred[2] and cmp_gold[3] in pred[3]) \\\n",
    "                            or (pred[2] in cmp_gold[2] and cmp_gold[3] in pred[3]) \\\n",
    "                            or (cmp_gold[2] in pred[2] and pred[3] in cmp_gold[3]):\n",
    "                            count_correct += 1\n",
    "                            break\n",
    "            count_correct = min(count_correct, golds_length)\n",
    "            count_wrong_from_gold = golds_length - count_correct\n",
    "            count_wrong_from_pred = preds_length - count_correct\n",
    "            dict_count[cur_gold_rel].append((count_correct, count_wrong_from_gold, count_wrong_from_pred))\n",
    "       \n",
    "    matrix = [v for _,v in dict_count.items()]\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_level(rel_extend_map, matrix):\n",
    "    value = [c for v in rel_extend_map.values() for c in v]\n",
    "    word_count_dict = {k:[] for k in value}\n",
    "\n",
    "    tmp_list = []\n",
    "\n",
    "    for idx, row in enumerate(matrix):\n",
    "        correct_sum = 0\n",
    "        wrong_from_gold_sum = 0\n",
    "        wrong_from_pred_sum = 0\n",
    "        for tuple in row:\n",
    "            correct_sum += tuple[0]\n",
    "            wrong_from_gold_sum += tuple[1]\n",
    "            wrong_from_pred_sum += tuple[2]\n",
    "        if correct_sum + wrong_from_pred_sum == 0:\n",
    "            P = 0.0\n",
    "        else:\n",
    "            P = correct_sum / (correct_sum + wrong_from_pred_sum)\n",
    "        if correct_sum + wrong_from_gold_sum == 0:\n",
    "            R = 0.0\n",
    "        else:\n",
    "            R = correct_sum / (correct_sum + wrong_from_gold_sum)\n",
    "        \n",
    "        if P + R == 0:\n",
    "            F1 = 0.0\n",
    "        else:\n",
    "            F1 = 2 * P * R / (P + R)\n",
    "        P_1 = 1 - P\n",
    "        R_1 = 1 - R\n",
    "        tmp_list.append((P, R, P_1, R_1, F1, correct_sum, wrong_from_gold_sum, wrong_from_pred_sum))\n",
    "\n",
    "\n",
    "    for idx,k in enumerate(word_count_dict.keys()):\n",
    "        word_count_dict[k] = tmp_list[idx]\n",
    "\n",
    "    return word_count_dict\n",
    "\n",
    "\n",
    "def word_level_gold(rel_extend_map, matrix, mode):\n",
    "    word_count_dict = {k:[] for k in rel_extend_map.keys()}\n",
    "\n",
    "    tmp_list = []\n",
    "\n",
    "    for idx, row in enumerate(matrix):\n",
    "        correct_sum = 0\n",
    "        wrong_from_gold_sum = 0\n",
    "        wrong_from_pred_sum = 0\n",
    "        for tuple in row:\n",
    "            correct_sum += tuple[0]\n",
    "            wrong_from_gold_sum += tuple[1]\n",
    "            wrong_from_pred_sum += tuple[2]\n",
    "        if correct_sum + wrong_from_pred_sum == 0:\n",
    "            P = 0.0\n",
    "        else:\n",
    "            P = correct_sum / (correct_sum + wrong_from_pred_sum)\n",
    "        if correct_sum + wrong_from_gold_sum == 0:\n",
    "            R = 0.0\n",
    "        else:\n",
    "            R = correct_sum / (correct_sum + wrong_from_gold_sum)\n",
    "        \n",
    "        if P + R == 0:\n",
    "            F1 = 0.0\n",
    "        else:\n",
    "            F1 = 2 * P * R / (P + R)\n",
    "        P_1 = 1 - P\n",
    "        R_1 = 1 - R\n",
    "        tmp_list.append((P, R, P_1, R_1, F1, correct_sum, wrong_from_gold_sum, wrong_from_pred_sum))\n",
    "\n",
    "\n",
    "    for idx,k in enumerate(word_count_dict.keys()):\n",
    "        word_count_dict[k] = tmp_list[idx]\n",
    "\n",
    "    df = pd.DataFrame.from_dict(word_count_dict, orient='index').reset_index()\n",
    "    df.columns = ['关系类型','P','R','1-P','1-R','F1','正确个数','golds中错误个数','preds中错误个数']\n",
    "    df.to_excel(f'./results/word_count_dict_gold_{mode}.xlsx',index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def word_dict2execl(word_count_dict, rel_extend_map, sim, mode):\n",
    "    df = pd.DataFrame.from_dict(word_count_dict, orient='index').reset_index()\n",
    "    df.columns = ['扩展关系词','P','R','1-P','1-R','F1','正确个数','golds中错误个数','preds中错误个数']\n",
    "    extend_rel_list = df['扩展关系词'].tolist()\n",
    "    gold_rel_list = [get_type(rel_extend_map, rel) for rel in extend_rel_list]\n",
    "    df['关系类型'] = gold_rel_list\n",
    "\n",
    "    cols = list(df.columns)\n",
    "    cols.insert(0, cols.pop(cols.index('关系类型')))\n",
    "    df = df.loc[:, cols]\n",
    "\n",
    "    if sim != None:\n",
    "        df.insert(loc=2, column='是否语义相关', value=sim)\n",
    "\n",
    "  \n",
    "    df.to_excel(f'./results/word_count_dict_{mode}.xlsx',index=False)\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_level(rel_extend_map, sim, matrix, input_length):\n",
    "    length = [len(value) for value in rel_extend_map.values()]\n",
    "\n",
    "    sentences_dict = {k:[] for k in range(input_length)}\n",
    "\n",
    "    l = length[0]\n",
    "    c = 0\n",
    "    t = -1\n",
    "\n",
    "    for idx,key in enumerate(sentences_dict.keys()):\n",
    "        if idx % 10 == 0:\n",
    "            t += 1\n",
    "            if idx == 0:\n",
    "                last_l = 0\n",
    "            else:\n",
    "                last_l += l\n",
    "            l = length[t]\n",
    "        sim_index = last_l\n",
    "        for row in matrix[last_l:last_l + l]:\n",
    "            tuple = row[idx % 10]\n",
    "            sentences_dict[key].append((tuple[0], tuple[1], tuple[2], sim[sim_index]))\n",
    "            c += 1\n",
    "            sim_index += 1\n",
    "            if c == l:\n",
    "                c = 0\n",
    "                break\n",
    "            \n",
    "    return sentences_dict\n",
    "\n",
    "\n",
    "def process_sim(sentences_dict, input_list2id, sim_or_not, input_length):\n",
    "    sentences_count_dict = {k:[] for k in range(input_length)}\n",
    "\n",
    "    sentence_matrix = []\n",
    "    for k,v in sentences_dict.items():\n",
    "        sentence_matrix.append(v)\n",
    "\n",
    "    tmp = []\n",
    "\n",
    "    for idx, row in enumerate(sentence_matrix):\n",
    "        correct_sum = 0\n",
    "        wrong_from_gold_sum = 0\n",
    "        wrong_from_pred_sum = 0\n",
    "        for tuple in row:\n",
    "            if tuple[3] == sim_or_not:\n",
    "                correct_sum += tuple[0]\n",
    "                wrong_from_gold_sum += tuple[1]\n",
    "                wrong_from_pred_sum += tuple[2]\n",
    "        if correct_sum + wrong_from_pred_sum == 0:\n",
    "            P = 0.0\n",
    "        else:\n",
    "            P = correct_sum / (correct_sum + wrong_from_pred_sum)\n",
    "        if correct_sum + wrong_from_gold_sum == 0:\n",
    "            R = 0.0\n",
    "        else:\n",
    "            R = correct_sum / (correct_sum + wrong_from_gold_sum)\n",
    "        P_1 = 1 - P\n",
    "        R_1 = 1 - R\n",
    "        tmp.append((sim_or_not, P, R, P_1, R_1, correct_sum, wrong_from_gold_sum, wrong_from_pred_sum))\n",
    "\n",
    "    for idx,k in enumerate(sentences_count_dict.keys()):\n",
    "        sentences_count_dict[k] = tmp[idx]\n",
    "    \n",
    "    sentence = input_list2id.copy()\n",
    "    for key in sentence.keys():\n",
    "        sentence[key] = sentences_count_dict[input_list2id[key]]\n",
    "\n",
    "    df = pd.DataFrame.from_dict(sentence, orient='index').reset_index()\n",
    "    df.columns = ['句子','是否语义相似','P','R','1-P','1-R','正确个数','golds中错误个数','preds中错误个数']\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def sentence_dict2execl(input_list, sentences_dict, input_length, mode):\n",
    "    input_list2id = {}\n",
    "    for idx, item in enumerate(input_list):\n",
    "        input_list2id[item] = idx\n",
    "    \n",
    "    df_no = process_sim(sentences_dict, input_list2id, 0, input_length)\n",
    "    df_yes = process_sim(sentences_dict, input_list2id, 1, input_length)  \n",
    "    concat_df = pd.concat([df_no, df_yes], axis=0)\n",
    "    concat_df_sorted = concat_df.sort_index()\n",
    "\n",
    "    group = concat_df_sorted.groupby('句子', sort=False)\n",
    "    merged_df = pd.DataFrame()\n",
    "    for _, group_df in group:\n",
    "        group_df = group_df.sort_values(by=['是否语义相似'], ascending=True)\n",
    "        merged_df = pd.concat([merged_df, group_df], axis=0)\n",
    "        \n",
    "    merged_df.to_excel(f'./results/sentence_count_dict_{mode}.xlsx',index=False)\n",
    "    \n",
    "    return merged_df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_level(word_count_dict):\n",
    "    grouped = word_count_dict.groupby('关系类型', sort=False)\n",
    "    merged_df = pd.DataFrame(columns=['关系类型', '是否语义相关', '正确个数', 'golds中错误个数', 'preds中错误个数'])\n",
    "\n",
    "    for ent_type, group_df in grouped:\n",
    "        zero_semantic_df = group_df[group_df['是否语义相关'] == 0]\n",
    "        one_semantic_df = group_df[group_df['是否语义相关'] == 1]\n",
    "\n",
    "        merged_row_0 = {\n",
    "            '关系类型': ent_type,\n",
    "            '是否语义相关': 0,\n",
    "            '正确个数': zero_semantic_df['正确个数'].sum(),\n",
    "            'golds中错误个数': zero_semantic_df['golds中错误个数'].sum(),\n",
    "            'preds中错误个数': zero_semantic_df['preds中错误个数'].sum()\n",
    "        }\n",
    "        merged_row_1 = {\n",
    "            '关系类型': ent_type,\n",
    "            '是否语义相关': 1,\n",
    "            '正确个数': one_semantic_df['正确个数'].sum(),\n",
    "            'golds中错误个数': one_semantic_df['golds中错误个数'].sum(),\n",
    "            'preds中错误个数': one_semantic_df['preds中错误个数'].sum()\n",
    "        }\n",
    "        \n",
    "        merged_df = merged_df._append(merged_row_0, ignore_index=True)\n",
    "        merged_df = merged_df._append(merged_row_1, ignore_index=True)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def calculate_scores(row):\n",
    "    correct = row['正确个数']\n",
    "    gold_errors = row['golds中错误个数']\n",
    "    preds_errors = row['preds中错误个数']\n",
    "    if correct + preds_errors == 0:\n",
    "        P = 0.0\n",
    "    else:\n",
    "        P = correct / (correct + preds_errors)\n",
    "    if correct + gold_errors == 0:\n",
    "        R = 0.0\n",
    "    else:\n",
    "        R = correct / (correct + gold_errors)\n",
    "    \n",
    "    P_1 = 1 - P\n",
    "    R_1 = 1 - R\n",
    "\n",
    "    return pd.Series([P, R, P_1, R_1])\n",
    "\n",
    "\n",
    "def count_dict2execl(merged_df, mode):\n",
    "    merged_df[['P','R','P_1','R_1']] = merged_df.apply(calculate_scores, axis=1)\n",
    "    merged_df.to_excel(f'./results/all_count_dict_{mode}.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CMeIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/cmeie/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/cmeie/final_rel_extend_map_zh.json','r',encoding='utf-8') as f:\n",
    "    rel_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/cmeie/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/cmeie/backward_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    backward = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/cmeie/backward_gold_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    backward_gold = [eval(line) for line in f]    \n",
    "\n",
    "with open('./data/cmeie/rel_sim_human_zh.json','r', encoding='utf-8') as f:\n",
    "    rel_sim_human = json.load(f)\n",
    "\n",
    "input_length = len(input_list)\n",
    "\n",
    "sim = []\n",
    "for key in rel_sim_human.keys():\n",
    "    for k,v in rel_sim_human[key].items():\n",
    "        sim.append(v)\n",
    "\n",
    "\n",
    "# matrix = get_count_matrix(rel_extend_map, merged_golds, backward)\n",
    "# word_count_dict = word_level(rel_extend_map, matrix)\n",
    "# df_word = word_dict2execl(word_count_dict, rel_extend_map, sim, mode='zh')\n",
    "\n",
    "# sentences_dict = sentence_level(rel_extend_map, sim, matrix,  input_length)\n",
    "# df_sent = sentence_dict2execl(input_list, sentences_dict, input_length, mode='zh')\n",
    "\n",
    "# all_count_dict = all_level(df_word)\n",
    "# count_dict2execl(all_count_dict, mode='zh')\n",
    "\n",
    "matrix = get_gold_count_matrix(rel_extend_map, merged_golds, backward_gold)\n",
    "df_word_gold= word_level_gold(rel_extend_map, matrix, mode='zh')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCIERC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/scierc/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/scierc/final_rel_extend_map_en.json','r',encoding='utf-8') as f:\n",
    "    rel_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/scierc/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/scierc/backward_en.txt', 'r', encoding='utf-8') as f:\n",
    "    backward = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/scierc/backward_gold_en.txt', 'r', encoding='utf-8') as f:\n",
    "    backward_gold = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/scierc/rel_sim_human_en.json','r', encoding='utf-8') as f:\n",
    "    rel_sim_human = json.load(f)\n",
    "\n",
    "input_length = len(input_list)\n",
    "\n",
    "sim = []\n",
    "for key in rel_sim_human.keys():\n",
    "    for k,v in rel_sim_human[key].items():\n",
    "        sim.append(v)\n",
    "\n",
    "\n",
    "# matrix = get_count_matrix(rel_extend_map, merged_golds, backward)\n",
    "# word_count_dict = word_level(rel_extend_map, matrix)\n",
    "# df_word = word_dict2execl(word_count_dict, rel_extend_map, sim, mode='en')\n",
    "\n",
    "# sentences_dict = sentence_level(rel_extend_map, sim, matrix,  input_length)\n",
    "# df_sent = sentence_dict2execl(input_list, sentences_dict, input_length, mode='en')\n",
    "\n",
    "# all_count_dict = all_level(df_word)\n",
    "# count_dict2execl(all_count_dict, mode='en')\n",
    "\n",
    "matrix = get_gold_count_matrix(rel_extend_map, merged_golds, backward_gold)\n",
    "df_word_gold= word_level_gold(rel_extend_map, matrix, mode='chatgpt_en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpaca-CMeIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/cmeie/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/cmeie/final_rel_extend_map_alpaca_33B_zh.json','r',encoding='utf-8') as f:\n",
    "    rel_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/cmeie/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/cmeie/backward_alpaca_33B_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    backward = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/cmeie/backward_gold_alpaca_33B_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    backward_gold = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/cmeie/rel_sim_human_alpaca_33B_zh.json','r', encoding='utf-8') as f:\n",
    "    rel_sim_human = json.load(f)\n",
    "\n",
    "input_length = len(input_list)\n",
    "\n",
    "sim = []\n",
    "for key in rel_sim_human.keys():\n",
    "    for k,v in rel_sim_human[key].items():\n",
    "        sim.append(v)\n",
    "\n",
    "\n",
    "# matrix = get_count_matrix(rel_extend_map, merged_golds, backward)\n",
    "# word_count_dict = word_level(rel_extend_map, matrix)\n",
    "# df_word = word_dict2execl(word_count_dict, rel_extend_map, sim, mode='alpaca_33B_zh')\n",
    "\n",
    "# sentences_dict = sentence_level(rel_extend_map, sim, matrix,  input_length)\n",
    "# df_sent = sentence_dict2execl(input_list, sentences_dict, input_length, mode='alpaca_33B_zh')\n",
    "\n",
    "# all_count_dict = all_level(df_word)\n",
    "# count_dict2execl(all_count_dict, mode='alpaca_33B_zh')\n",
    "\n",
    "matrix = get_gold_count_matrix(rel_extend_map, merged_golds, backward_gold)\n",
    "df_word_gold= word_level_gold(rel_extend_map, matrix, mode='alpaca_33B_zh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpaca-SCIERC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/scierc/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/scierc/final_rel_extend_map_alpaca_33B_en.json','r',encoding='utf-8') as f:\n",
    "    rel_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/scierc/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/scierc/backward_alpaca_33B_en.txt', 'r', encoding='utf-8') as f:\n",
    "    backward = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/scierc/backward_gold_alpaca_33B_en.txt', 'r', encoding='utf-8') as f:\n",
    "    backward_gold = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/scierc/rel_sim_human_alpaca_33B_en.json','r', encoding='utf-8') as f:\n",
    "    rel_sim_human = json.load(f)\n",
    "\n",
    "input_length = len(input_list)\n",
    "\n",
    "sim = []\n",
    "for key in rel_sim_human.keys():\n",
    "    for k,v in rel_sim_human[key].items():\n",
    "        sim.append(v)\n",
    "\n",
    "\n",
    "# matrix = get_count_matrix(rel_extend_map, merged_golds, backward)\n",
    "# word_count_dict = word_level(rel_extend_map, matrix)\n",
    "# df_word = word_dict2execl(word_count_dict, rel_extend_map, sim, mode='alpaca_33B_en')\n",
    "\n",
    "# sentences_dict = sentence_level(rel_extend_map, sim, matrix,  input_length)\n",
    "# df_sent = sentence_dict2execl(input_list, sentences_dict, input_length, mode='alpaca_33B_en')\n",
    "\n",
    "# all_count_dict = all_level(df_word)\n",
    "# count_dict2execl(all_count_dict, mode='alpaca_33B_en')\n",
    "\n",
    "matrix = get_gold_count_matrix(rel_extend_map, merged_golds, backward_gold)\n",
    "df_word_gold= word_level_gold(rel_extend_map, matrix, mode='alpaca_33B_en')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llama2-SCIERC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/scierc/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/scierc/final_rel_extend_map_llama2_70B_en.json','r',encoding='utf-8') as f:\n",
    "    rel_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/scierc/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/scierc/backward_llama2_70B_en.txt', 'r', encoding='utf-8') as f:\n",
    "    backward = [eval(line) for line in f]\n",
    " \n",
    "with open('./data/scierc/backward_gold_llama2_70B_en.txt', 'r', encoding='utf-8') as f:\n",
    "    backward_gold = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/scierc/rel_sim_human_llama2_70B_en.json','r', encoding='utf-8') as f:\n",
    "    rel_sim_human = json.load(f)\n",
    "\n",
    "input_length = len(input_list)\n",
    "\n",
    "sim = []\n",
    "for key in rel_sim_human.keys():\n",
    "    for k,v in rel_sim_human[key].items():\n",
    "        sim.append(v)\n",
    "\n",
    "\n",
    "# matrix = get_count_matrix(rel_extend_map, merged_golds, backward)\n",
    "# word_count_dict = word_level(rel_extend_map, matrix)\n",
    "# df_word = word_dict2execl(word_count_dict, rel_extend_map, sim, mode='llama2_70B_en')\n",
    "# # df_word = word_dict2execl(word_count_dict, rel_extend_map, None, mode='llama2_70B_en')\n",
    "\n",
    "# sentences_dict = sentence_level(rel_extend_map, sim, matrix,  input_length)\n",
    "# df_sent = sentence_dict2execl(input_list, sentences_dict, input_length, mode='llama2_70B_en')\n",
    "\n",
    "# all_count_dict = all_level(df_word)\n",
    "# count_dict2execl(all_count_dict, mode='llama2_70B_en')\n",
    "\n",
    "matrix = get_gold_count_matrix(rel_extend_map, merged_golds, backward_gold)\n",
    "df_word_gold= word_level_gold(rel_extend_map, matrix, mode='llama2_70B_en')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGLM-CMeIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/cmeie/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/cmeie/final_rel_extend_map_chatglm_6B_zh.json','r',encoding='utf-8') as f:\n",
    "    rel_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/cmeie/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/cmeie/backward_chatglm_6B_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    backward = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/cmeie/backward_gold_chatglm_6B_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    backward_gold = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/cmeie/rel_sim_human_chatglm_6B_zh.json','r', encoding='utf-8') as f:\n",
    "    rel_sim_human = json.load(f)\n",
    "\n",
    "input_length = len(input_list)\n",
    "\n",
    "sim = []\n",
    "for key in rel_sim_human.keys():\n",
    "    for k,v in rel_sim_human[key].items():\n",
    "        sim.append(v)\n",
    "\n",
    "\n",
    "# matrix = get_count_matrix(rel_extend_map, merged_golds, backward)\n",
    "# word_count_dict = word_level(rel_extend_map, matrix)\n",
    "# df_word = word_dict2execl(word_count_dict, rel_extend_map, sim, mode='chatglm_6B_zh')\n",
    "# # df_word = word_dict2execl(word_count_dict, rel_extend_map, None, mode='chatglm_6B_zh')\n",
    "\n",
    "# sentences_dict = sentence_level(rel_extend_map, sim, matrix,  input_length)\n",
    "# df_sent = sentence_dict2execl(input_list, sentences_dict, input_length, mode='chatglm_6B_zh')\n",
    "\n",
    "# all_count_dict = all_level(df_word)\n",
    "# count_dict2execl(all_count_dict, mode='chatglm_6B_zh')\n",
    "matrix = get_gold_count_matrix(rel_extend_map, merged_golds, backward_gold)\n",
    "df_word_gold= word_level_gold(rel_extend_map, matrix, mode='chatglm_6B_zh')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGLM-SCIERC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/scierc/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/scierc/final_rel_extend_map_chatglm_6B_en.json','r',encoding='utf-8') as f:\n",
    "    rel_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/scierc/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/scierc/backward_chatglm_6B_en.txt', 'r', encoding='utf-8') as f:\n",
    "    backward = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/scierc/backward_gold_chatglm_6B_en.txt', 'r', encoding='utf-8') as f:\n",
    "    backward_gold = [eval(line) for line in f]\n",
    "    \n",
    "with open('./data/scierc/rel_sim_human_chatglm_6B_en.json','r', encoding='utf-8') as f:\n",
    "    rel_sim_human = json.load(f)\n",
    "\n",
    "input_length = len(input_list)\n",
    "\n",
    "sim = []\n",
    "for key in rel_sim_human.keys():\n",
    "    for k,v in rel_sim_human[key].items():\n",
    "        sim.append(v)\n",
    "\n",
    "\n",
    "# matrix = get_count_matrix(rel_extend_map, merged_golds, backward)\n",
    "# word_count_dict = word_level(rel_extend_map, matrix)\n",
    "# df_word = word_dict2execl(word_count_dict, rel_extend_map, sim, mode='chatglm_6B_en')\n",
    "# # df_word = word_dict2execl(word_count_dict, rel_extend_map, None, mode='llama2_70B_en')\n",
    "\n",
    "# sentences_dict = sentence_level(rel_extend_map, sim, matrix,  input_length)\n",
    "# df_sent = sentence_dict2execl(input_list, sentences_dict, input_length, mode='chatglm_6B_en')\n",
    "\n",
    "# all_count_dict = all_level(df_word)\n",
    "# count_dict2execl(all_count_dict, mode='chatglm_6B_en')\n",
    "matrix = get_gold_count_matrix(rel_extend_map, merged_golds, backward_gold)\n",
    "df_word_gold= word_level_gold(rel_extend_map, matrix, mode='chatglm_6B_en')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT4-CMeIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/cmeie/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "input_list = input_list[40:50]\n",
    "\n",
    "with open('./data/cmeie/final_rel_extend_map_gpt4_zh.json','r',encoding='utf-8') as f:\n",
    "    rel_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/cmeie/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "merged_golds = merged_golds[40:50]\n",
    "\n",
    "with open('./data/cmeie/backward_gpt4_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    backward = [eval(line) for line in f]\n",
    "    \n",
    "with open('./data/cmeie/rel_sim_human_gpt4_zh.json','r', encoding='utf-8') as f:\n",
    "    rel_sim_human = json.load(f)\n",
    "\n",
    "input_length = len(input_list)\n",
    "\n",
    "sim = []\n",
    "for key in rel_sim_human.keys():\n",
    "    for k,v in rel_sim_human[key].items():\n",
    "        sim.append(v)\n",
    "\n",
    "\n",
    "matrix = get_count_matrix(rel_extend_map, merged_golds, backward)\n",
    "word_count_dict = word_level(rel_extend_map, matrix)\n",
    "df_word = word_dict2execl(word_count_dict, rel_extend_map, sim, mode='gpt4_zh')\n",
    "# df_word = word_dict2execl(word_count_dict, rel_extend_map, None, mode='chatglm_6B_zh')\n",
    "\n",
    "sentences_dict = sentence_level(rel_extend_map, sim, matrix,  input_length)\n",
    "df_sent = sentence_dict2execl(input_list, sentences_dict, input_length, mode='gpt4_zh')\n",
    "\n",
    "all_count_dict = all_level(df_word)\n",
    "count_dict2execl(all_count_dict, mode='gpt4_zh')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT4-SCIERC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/scierc/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "input_list = input_list[10:20]\n",
    "\n",
    "with open('./data/scierc/final_rel_extend_map_gpt4_en.json','r',encoding='utf-8') as f:\n",
    "    rel_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/scierc/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "merged_golds = merged_golds[10:20]\n",
    "\n",
    "with open('./data/scierc/backward_gpt4_en.txt', 'r', encoding='utf-8') as f:\n",
    "    backward = [eval(line) for line in f]\n",
    "    \n",
    "with open('./data/scierc/rel_sim_human_gpt4_en.json','r', encoding='utf-8') as f:\n",
    "    rel_sim_human = json.load(f)\n",
    "\n",
    "input_length = len(input_list)\n",
    "\n",
    "sim = []\n",
    "for key in rel_sim_human.keys():\n",
    "    for k,v in rel_sim_human[key].items():\n",
    "        sim.append(v)\n",
    "\n",
    "\n",
    "matrix = get_count_matrix(rel_extend_map, merged_golds, backward)\n",
    "word_count_dict = word_level(rel_extend_map, matrix)\n",
    "df_word = word_dict2execl(word_count_dict, rel_extend_map, sim, mode='gpt4_en')\n",
    "# df_word = word_dict2execl(word_count_dict, rel_extend_map, None, mode='llama2_70B_en')\n",
    "\n",
    "sentences_dict = sentence_level(rel_extend_map, sim, matrix,  input_length)\n",
    "df_sent = sentence_dict2execl(input_list, sentences_dict, input_length, mode='gpt4_en')\n",
    "\n",
    "all_count_dict = all_level(df_word)\n",
    "count_dict2execl(all_count_dict, mode='gpt4_en')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baichuan2-CMeIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/cmeie/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/cmeie/final_rel_extend_map_baichuan2_13B_zh.json','r',encoding='utf-8') as f:\n",
    "    rel_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/cmeie/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/cmeie/backward_baichuan2_13B_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    backward = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/cmeie/backward_gold_baichuan2_13B_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    backward_gold = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/cmeie/rel_sim_human_baichuan2_13B_zh.json','r', encoding='utf-8') as f:\n",
    "    rel_sim_human = json.load(f)\n",
    "\n",
    "input_length = len(input_list)\n",
    "\n",
    "sim = []\n",
    "for key in rel_sim_human.keys():\n",
    "    for k,v in rel_sim_human[key].items():\n",
    "        sim.append(v)\n",
    "\n",
    "\n",
    "matrix = get_count_matrix(rel_extend_map, merged_golds, backward)\n",
    "word_count_dict = word_level(rel_extend_map, matrix)\n",
    "df_word = word_dict2execl(word_count_dict, rel_extend_map, sim, mode='baichuan2_13B_zh')\n",
    "\n",
    "sentences_dict = sentence_level(rel_extend_map, sim, matrix,  input_length)\n",
    "df_sent = sentence_dict2execl(input_list, sentences_dict, input_length, mode='baichuan2_13B_zh')\n",
    "\n",
    "all_count_dict = all_level(df_word)\n",
    "count_dict2execl(all_count_dict, mode='baichuan2_13B_zh')\n",
    "# matrix = get_gold_count_matrix(rel_extend_map, merged_golds, backward_gold)\n",
    "# df_word_gold= word_level_gold(rel_extend_map, matrix, mode='baichuan2_13B_zh')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baichuan2-SCIERC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/scierc/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/scierc/final_rel_extend_map_baichuan2_13B_en copy.json','r',encoding='utf-8') as f:\n",
    "    rel_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/scierc/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/scierc/backward_baichuan2_13B_en.txt', 'r', encoding='utf-8') as f:\n",
    "    backward = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/scierc/backward_gold_baichuan2_13B_en.txt', 'r', encoding='utf-8') as f:\n",
    "    backward_gold = [eval(line) for line in f]\n",
    "    \n",
    "with open('./data/scierc/rel_sim_human_baichuan2_13B_en copy.json','r', encoding='utf-8') as f:\n",
    "    rel_sim_human = json.load(f)\n",
    "\n",
    "input_length = len(input_list)\n",
    "\n",
    "sim = []\n",
    "for key in rel_sim_human.keys():\n",
    "    for k,v in rel_sim_human[key].items():\n",
    "        sim.append(v)\n",
    "\n",
    "\n",
    "matrix = get_count_matrix(rel_extend_map, merged_golds, backward)\n",
    "word_count_dict = word_level(rel_extend_map, matrix)\n",
    "df_word = word_dict2execl(word_count_dict, rel_extend_map, sim, mode='baichuan2_13B_en')\n",
    "\n",
    "sentences_dict = sentence_level(rel_extend_map, sim, matrix,  input_length)\n",
    "df_sent = sentence_dict2execl(input_list, sentences_dict, input_length, mode='baichuan2_13B_en')\n",
    "\n",
    "all_count_dict = all_level(df_word)\n",
    "count_dict2execl(all_count_dict, mode='baichuan2_13B_en')\n",
    "# matrix = get_gold_count_matrix(rel_extend_map, merged_golds, backward_gold)\n",
    "# df_word_gold= word_level_gold(rel_extend_map, matrix, mode='baichuan2_13B_en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
