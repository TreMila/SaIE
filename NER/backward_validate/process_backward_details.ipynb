{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_details(backward_details):\n",
    "    merged_list = []\n",
    "    current_item = \"\"\n",
    "\n",
    "    for i, item in enumerate(backward_details):\n",
    "        if item.strip() and item.strip()[0].isdigit():\n",
    "            j = 1\n",
    "            while j < len(item.strip()) and item.strip()[j].isdigit():\n",
    "                j += 1\n",
    "            if j < len(item.strip()) and item.strip()[j] == \"：\":\n",
    "                merged_list.append(current_item)\n",
    "                current_item = item.strip()\n",
    "            else:\n",
    "                current_item += item.strip()\n",
    "        else:\n",
    "            current_item += item.strip()\n",
    "\n",
    "    merged_list.append(current_item)\n",
    "    merged_list.pop(0)\n",
    "    return merged_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tuples(string):\n",
    "    tuples = []\n",
    "    depth = 0\n",
    "    start = None\n",
    "    for i, char in enumerate(string):\n",
    "        if char == \"(\" or char == \"（\" or char == \"[\":\n",
    "            if depth == 0:\n",
    "                start = i\n",
    "            depth += 1\n",
    "        elif char == \")\" or char == \"）\" or char == \"]\":\n",
    "            depth -= 1\n",
    "            if depth == 0 and start is not None:\n",
    "                tuples.append(string[start+1:i])\n",
    "                start = None\n",
    "    return tuples\n",
    "\n",
    "\n",
    "def get_results(merged_list, input_list, entity_type_list, ent_extend_map, mode):\n",
    "    input_list2ent = {}\n",
    "    for idx, item in enumerate(input_list):\n",
    "        input_list2ent[item] = entity_type_list[idx//10]\n",
    "\n",
    "    ent_extend_index = 0\n",
    "    cur_ent_extend_list = []\n",
    "    A = []\n",
    "\n",
    "    for idx, item in enumerate(merged_list):\n",
    "        B = []\n",
    "        cur_output_3 = item.split('Output:')\n",
    "        cur_idx = cur_output_3[0].split('：')[0]\n",
    "        cur_example = input_list[int(cur_idx)-1]\n",
    "        cur_true_ent = input_list2ent[cur_example]\n",
    "        if ent_extend_index == len(cur_ent_extend_list):\n",
    "            ent_extend_index = 0\n",
    "\n",
    "        cur_ent_extend_list = ent_extend_map[cur_true_ent]\n",
    "        cur_extend_ent = cur_ent_extend_list[ent_extend_index]\n",
    "\n",
    "        for output in cur_output_3[1:]:\n",
    "            sep = \"\"\n",
    "            if 'zh' in mode:\n",
    "                output = output.replace(\"：\", \":\")\n",
    "                if \"生成实体列表:\" in output:\n",
    "                    sep = \"生成实体列表:\"\n",
    "                elif \"生成实体列表：\" in output:\n",
    "                    sep = \"生成实体列表：\"\n",
    "            elif 'en' in mode:\n",
    "                if \"Entity list:\" in output:\n",
    "                    sep = \"Entity list:\"\n",
    "                elif \"entity list:\" in output:\n",
    "                    sep = \"entity list:\"\n",
    "                elif \"Entity List:\" in output:\n",
    "                    sep = \"Entity List:\"\n",
    "                elif \"Generate an entity list:\" in output:\n",
    "                    sep = \"Generate an entity list:\"\n",
    "                elif \"Generated entity list:\" in output:\n",
    "                    sep = \"Generated entity list:\"\n",
    "                elif \"generate entity list:\" in output:\n",
    "                    sep = \"generate entity list:\"\n",
    "\n",
    "            if sep:\n",
    "                tmp = output.split(sep)[1]\n",
    "                matches = extract_tuples(tmp)\n",
    "                if matches:\n",
    "                    for match in matches:\n",
    "                        if 'zh' in mode:\n",
    "                            triple = tuple(match.replace('，',',').strip().strip('（）').split(','))\n",
    "                        elif 'en' in mode:\n",
    "                            triple = tuple(match.strip().strip('()').split(','))\n",
    "                        if len(triple) == 2:\n",
    "                            B.append((int(cur_idx)-1, cur_extend_ent, triple[1].strip()))\n",
    "                        else:\n",
    "                            B.append((int(cur_idx)-1, cur_extend_ent, ''))\n",
    "                else:\n",
    "                    B.append((int(cur_idx)-1, cur_extend_ent, ''))\n",
    "                \n",
    "            else:\n",
    "                B.append((int(cur_idx)-1, cur_extend_ent, ''))\n",
    "                continue\n",
    "        if B:\n",
    "            A.append(B)    \n",
    "\n",
    "        ent_extend_index += 1\n",
    "\n",
    "    A = [list(set(item)) for item in A]\n",
    "    \n",
    "    with open(f'./results/backward_{mode}.txt','w',encoding='utf-8') as f:\n",
    "        for item in A:\n",
    "            f.write(str(item)+'\\n')\n",
    "\n",
    "\n",
    "def get_gold_results(merged_list, input_list, entity_type_list, mode):\n",
    "    input_list2ent = {}\n",
    "    for idx, item in enumerate(input_list):\n",
    "        input_list2ent[item] = entity_type_list[idx//10]\n",
    "\n",
    "    A = []\n",
    "\n",
    "    for idx, item in enumerate(merged_list):\n",
    "        B = []\n",
    "        cur_output_3 = item.split('Output:')\n",
    "        cur_idx = cur_output_3[0].split('：')[0]\n",
    "        cur_example = input_list[int(cur_idx)-1]\n",
    "        cur_true_ent = input_list2ent[cur_example]\n",
    "\n",
    "\n",
    "        for output in cur_output_3[1:]:\n",
    "            sep = \"\"\n",
    "            if 'zh' in mode:\n",
    "                output = output.replace(\"：\", \":\")\n",
    "                if \"生成实体列表:\" in output:\n",
    "                    sep = \"生成实体列表:\"\n",
    "                elif \"生成实体列表：\" in output:\n",
    "                    sep = \"生成实体列表：\"\n",
    "            elif 'en' in mode:\n",
    "                if \"Entity list:\" in output:\n",
    "                    sep = \"Entity list:\"\n",
    "                elif \"entity list:\" in output:\n",
    "                    sep = \"entity list:\"\n",
    "                elif \"Entity List:\" in output:\n",
    "                    sep = \"Entity List:\"\n",
    "                elif \"Generate an entity list:\" in output:\n",
    "                    sep = \"Generate an entity list:\"\n",
    "                elif \"Generated entity list:\" in output:\n",
    "                    sep = \"Generated entity list:\"\n",
    "                elif \"generate entity list:\" in output:\n",
    "                    sep = \"generate entity list:\"\n",
    "\n",
    "            if sep:\n",
    "                tmp = output.split(sep)[1]\n",
    "                matches = extract_tuples(tmp)\n",
    "                if matches:\n",
    "                    for match in matches:\n",
    "                        if 'zh' in mode:\n",
    "                            triple = tuple(match.replace('，',',').strip().strip('（）').split(','))\n",
    "                        elif 'en' in mode:\n",
    "                            triple = tuple(match.strip().strip('()').split(','))\n",
    "                        if len(triple) == 2:\n",
    "                            B.append((int(cur_idx)-1, cur_true_ent, triple[1].strip()))\n",
    "                        else:\n",
    "                            B.append((int(cur_idx)-1, cur_true_ent, ''))\n",
    "                else:\n",
    "                    B.append((int(cur_idx)-1, cur_true_ent, ''))\n",
    "                \n",
    "            else:\n",
    "                B.append((int(cur_idx)-1, cur_true_ent, ''))\n",
    "                continue\n",
    "        if B:\n",
    "            A.append(B)    \n",
    "\n",
    "    A = [list(set(item)) for item in A]\n",
    "    \n",
    "    with open(f'./results/backward_gold_{mode}.txt','w',encoding='utf-8') as f:\n",
    "        for item in A:\n",
    "            f.write(str(item)+'\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CMeEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results/details_zh.txt','r',encoding='utf-8') as f:\n",
    "    backward_details = f.readlines()\n",
    "\n",
    "with open('./results/details_gold_chatgpt_zh.txt','r',encoding='utf-8') as f:\n",
    "    backward_gold_details = f.readlines()\n",
    "\n",
    "with open('./data/cmeee/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/cmeee/final_entity_extend_map_zh.json','r',encoding='utf-8') as f:\n",
    "    ent_extend_map = json.load(f)\n",
    "\n",
    "entity_type_list = ['dru', 'bod', 'pro', 'sym', 'equ', 'ite', 'dep', 'mic', 'dis']\n",
    "\n",
    "# merged_list = merge_details(backward_details)\n",
    "# get_results(merged_list, input_list, entity_type_list, ent_extend_map, 'zh')\n",
    "merged_list = merge_details(backward_gold_details)\n",
    "get_gold_results(merged_list, input_list, entity_type_list, 'chatgpt_zh')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACE05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results/details_en.txt','r',encoding='utf-8') as f:\n",
    "    backward_details = f.readlines()\n",
    "\n",
    "with open('./results/details_gold_chatgpt_en.txt','r',encoding='utf-8') as f:\n",
    "    backward_gold_details = f.readlines()\n",
    "\n",
    "with open('./data/ace05/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/ace05/final_entity_extend_map_en.json','r',encoding='utf-8') as f:\n",
    "    ent_extend_map = json.load(f)\n",
    "\n",
    "with open('../ACE05/labels.json','r',encoding='utf-8') as f:\n",
    "    entity_type_list = json.load(f)\n",
    "\n",
    "# merged_list = merge_details(backward_details)\n",
    "# get_results(merged_list, input_list, entity_type_list, ent_extend_map, 'en')\n",
    "\n",
    "merged_list = merge_details(backward_gold_details)\n",
    "get_gold_results(merged_list, input_list, entity_type_list, 'chatgpt_en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpaca-CMeEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理反向验证的输出文件\n",
    "with open('./results/details_alpaca_33B_zh.txt','r',encoding='utf-8') as f:\n",
    "    backward_details = f.readlines()\n",
    "\n",
    "with open('./results/details_gold_alpaca_33B_zh.txt','r',encoding='utf-8') as f:\n",
    "    backward_gold_details = f.readlines()\n",
    "\n",
    "with open('./data/cmeee/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/cmeee/final_entity_extend_map_alpaca_33B_zh.json','r',encoding='utf-8') as f:\n",
    "    ent_extend_map = json.load(f)\n",
    "\n",
    "entity_type_list = ['dru', 'bod', 'pro', 'sym', 'equ', 'ite', 'dep', 'mic', 'dis']\n",
    "\n",
    "# merged_list = merge_details(backward_details)\n",
    "# get_results(merged_list, input_list, entity_type_list, ent_extend_map, 'alpaca_33B_zh')\n",
    "merged_list = merge_details(backward_gold_details)\n",
    "get_gold_results(merged_list, input_list, entity_type_list, 'alpaca_33B_zh')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpaca-ACE05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results/details_alpaca_33B_en.txt','r',encoding='utf-8') as f:\n",
    "    backward_details = f.readlines()\n",
    "\n",
    "with open('./results/details_gold_alpaca_33B_en.txt','r',encoding='utf-8') as f:\n",
    "    backward_gold_details = f.readlines()\n",
    "\n",
    "with open('./data/ace05/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/ace05/final_entity_extend_map_alpaca_33B_en.json','r',encoding='utf-8') as f:\n",
    "    ent_extend_map = json.load(f)\n",
    "\n",
    "with open('../ACE05/labels.json','r',encoding='utf-8') as f:\n",
    "    entity_type_list = json.load(f)\n",
    "\n",
    "# merged_list = merge_details(backward_details)\n",
    "# get_results(merged_list, input_list, entity_type_list, ent_extend_map, 'alpaca_33B_en')\n",
    "# sum([len(ent_extend_map[key]) for key in entity_type_list])\n",
    "\n",
    "merged_list = merge_details(backward_gold_details)\n",
    "get_gold_results(merged_list, input_list, entity_type_list, 'alpaca_33B_en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llama2-ACE05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results/details_llama2_70B_en.txt','r',encoding='utf-8') as f:\n",
    "    backward_details = f.readlines()\n",
    "\n",
    "with open('./results/details_gold_llama2_70B_en.txt','r',encoding='utf-8') as f:\n",
    "    backward_gold_details = f.readlines()\n",
    "\n",
    "with open('./data/ace05/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/ace05/final_entity_extend_map_llama2_70B_en.json','r',encoding='utf-8') as f:\n",
    "    ent_extend_map = json.load(f)\n",
    "\n",
    "with open('../ACE05/labels.json','r',encoding='utf-8') as f:\n",
    "    entity_type_list = json.load(f)\n",
    "\n",
    "# merged_list = merge_details(backward_details)\n",
    "# get_results(merged_list, input_list, entity_type_list, ent_extend_map, 'llama2_70B_en')\n",
    "# sum([len(ent_extend_map[key]) for key in entity_type_list])\n",
    "\n",
    "merged_list = merge_details(backward_gold_details)\n",
    "get_gold_results(merged_list, input_list, entity_type_list, 'llama2_70B_en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGLM-CMeEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理反向验证的输出文件\n",
    "with open('./results/details_chatglm_6B_zh.txt','r',encoding='utf-8') as f:\n",
    "    backward_details = f.readlines()\n",
    "\n",
    "with open('./results/details_gold_chatglm_6B_zh.txt','r',encoding='utf-8') as f:\n",
    "    backward_gold_details = f.readlines()\n",
    "\n",
    "with open('./data/cmeee/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/cmeee/final_entity_extend_map_chatglm_6B_zh.json','r',encoding='utf-8') as f:\n",
    "    ent_extend_map = json.load(f)\n",
    "\n",
    "entity_type_list = ['dru', 'bod', 'pro', 'sym', 'equ', 'ite', 'dep', 'mic', 'dis']\n",
    "\n",
    "# merged_list = merge_details(backward_details)\n",
    "# get_results(merged_list, input_list, entity_type_list, ent_extend_map, 'chatglm_6B_zh')\n",
    "merged_list = merge_details(backward_gold_details)\n",
    "get_gold_results(merged_list, input_list, entity_type_list, 'chatglm_6B_zh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGLM-ACE05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results/details_chatglm_6B_en.txt','r',encoding='utf-8') as f:\n",
    "    backward_details = f.readlines()\n",
    "\n",
    "with open('./results/details_gold_chatglm_6B_en.txt','r',encoding='utf-8') as f:\n",
    "    backward_gold_details = f.readlines()\n",
    "\n",
    "with open('./data/ace05/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/ace05/final_entity_extend_map_chatglm_6B_en.json','r',encoding='utf-8') as f:\n",
    "    ent_extend_map = json.load(f)\n",
    "\n",
    "with open('../ACE05/labels.json','r',encoding='utf-8') as f:\n",
    "    entity_type_list = json.load(f)\n",
    "\n",
    "# merged_list = merge_details(backward_details)\n",
    "# get_results(merged_list, input_list, entity_type_list, ent_extend_map, 'chatglm_6B_en')\n",
    "# sum([len(ent_extend_map[key]) for key in entity_type_list])\n",
    "merged_list = merge_details(backward_gold_details)\n",
    "get_gold_results(merged_list, input_list, entity_type_list, 'chatglm_6B_en')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT4-CMeEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理反向验证的输出文件\n",
    "with open('./results/details_gpt4_zh.txt','r',encoding='utf-8') as f:\n",
    "    backward_details = f.readlines()\n",
    "\n",
    "with open('./data/cmeee/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/cmeee/final_entity_extend_map_gpt4_zh.json','r',encoding='utf-8') as f:\n",
    "    ent_extend_map = json.load(f)\n",
    "\n",
    "entity_type_list = ['dru', 'bod', 'pro', 'sym', 'equ', 'ite', 'dep', 'mic', 'dis']\n",
    "\n",
    "merged_list = merge_details(backward_details)\n",
    "get_results(merged_list, input_list, entity_type_list, ent_extend_map, 'gpt4_zh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT4-ACE05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results/details_gpt4_en.txt','r',encoding='utf-8') as f:\n",
    "    backward_details = f.readlines()\n",
    "\n",
    "with open('./data/ace05/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/ace05/final_entity_extend_map_gpt4_en.json','r',encoding='utf-8') as f:\n",
    "    ent_extend_map = json.load(f)\n",
    "\n",
    "with open('../ACE05/labels.json','r',encoding='utf-8') as f:\n",
    "    entity_type_list = json.load(f)\n",
    "\n",
    "merged_list = merge_details(backward_details)\n",
    "get_results(merged_list, input_list, entity_type_list, ent_extend_map, 'gpt4_en')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baichuan-CMeEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理反向验证的输出文件\n",
    "with open('./results/details_baichuan2_13B_zh.txt','r',encoding='utf-8') as f:\n",
    "    backward_details = f.readlines()\n",
    "\n",
    "with open('./results/details_gold_baichuan2_13B_zh.txt','r',encoding='utf-8') as f:\n",
    "    backward_gold_details = f.readlines()\n",
    "\n",
    "with open('./data/cmeee/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/cmeee/final_entity_extend_map_baichuan2_13B_zh.json','r',encoding='utf-8') as f:\n",
    "    ent_extend_map = json.load(f)\n",
    "\n",
    "entity_type_list = ['dru', 'bod', 'pro', 'sym', 'equ', 'ite', 'dep', 'mic', 'dis']\n",
    "\n",
    "# merged_list = merge_details(backward_details)\n",
    "# get_results(merged_list, input_list, entity_type_list, ent_extend_map, 'baichuan2_13B_zh')\n",
    "merged_list = merge_details(backward_gold_details)\n",
    "get_gold_results(merged_list, input_list, entity_type_list, 'baichuan2_13B_zh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baichuan-ACE05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results/details_baichuan2_13B_en.txt','r',encoding='utf-8') as f:\n",
    "    backward_details = f.readlines()\n",
    "\n",
    "with open('./results/details_gold_baichuan2_13B_en.txt','r',encoding='utf-8') as f:\n",
    "    backward_gold_details = f.readlines()\n",
    "\n",
    "with open('./data/ace05/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/ace05/final_entity_extend_map_baichuan2_13B_en.json','r',encoding='utf-8') as f:\n",
    "    ent_extend_map = json.load(f)\n",
    "\n",
    "with open('../ACE05/labels.json','r',encoding='utf-8') as f:\n",
    "    entity_type_list = json.load(f)\n",
    "\n",
    "# merged_list = merge_details(backward_details)\n",
    "# get_results(merged_list, input_list, entity_type_list, ent_extend_map, 'baichuan2_13B_en')\n",
    "# sum([len(ent_extend_map[key]) for key in entity_type_list])\n",
    "merged_list = merge_details(backward_gold_details)\n",
    "get_gold_results(merged_list, input_list, entity_type_list, 'baichuan2_13B_en')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
