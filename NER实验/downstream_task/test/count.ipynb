{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T10:52:24.075895Z",
     "start_time": "2023-12-11T10:52:24.070683Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T10:52:24.632121Z",
     "start_time": "2023-12-11T10:52:24.614032Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_type(extend_map, label):\n",
    "    for key, val in extend_map.items():\n",
    "        for v in val:\n",
    "            if v == label:\n",
    "                return key\n",
    "\n",
    "\n",
    "def get_count_matrix(ent_extend_map, entity_type_dict, merged_golds, test, mode):\n",
    "    new_ent_extend_map = {k:[] for k in ent_extend_map.keys()}\n",
    "    for k in new_ent_extend_map.keys():\n",
    "        # new_ent_extend_map[k] = [k] + ent_extend_map[k]\n",
    "        new_ent_extend_map[k] = ent_extend_map[k]\n",
    "    value = [c for v in new_ent_extend_map.values() for c in v]\n",
    "    dict = {k: [] for k in value}\n",
    "    dict_count = {k: [] for k in value}\n",
    "\n",
    "    for item in test:\n",
    "        lab = item[0][1]\n",
    "        dict[lab].append(item)\n",
    "\n",
    "    for k,v in dict.items():   \n",
    "        cur_extend_ent = k\n",
    "        cur_list = v\n",
    "        for item in cur_list:\n",
    "            count_correct = 0\n",
    "            count_wrong_from_gold = 0\n",
    "            count_wrong_from_pred = 0\n",
    "\n",
    "            cur_gold_ent = get_type(new_ent_extend_map, cur_extend_ent)\n",
    "            cur_idx = item[0][0]\n",
    "            cur_merged_golds = merged_golds[cur_idx]\n",
    "            if 'zh' in mode:\n",
    "                cur_golds = [gold for gold in cur_merged_golds if entity_type_dict[gold[1]] == cur_gold_ent]\n",
    "            elif 'en' in mode:\n",
    "                cur_golds = [gold for gold in cur_merged_golds if gold[1] == cur_gold_ent]\n",
    "\n",
    "            cur_preds = [pred for pred in item] \n",
    "\n",
    "            preds_length = len(cur_preds)\n",
    "            golds_length = len(cur_golds)\n",
    "\n",
    "            if golds_length != 0:\n",
    "                for pred in cur_preds:\n",
    "                    for cmp_gold in cur_golds:\n",
    "                        if pred[2] == '':\n",
    "                            preds_length -= 1\n",
    "                            break\n",
    "                        elif pred[2] in cmp_gold[2] or cmp_gold[2] in pred[2]:\n",
    "                            count_correct += 1\n",
    "                            break\n",
    "            count_correct = min(count_correct, golds_length)\n",
    "            count_wrong_from_gold = golds_length - count_correct\n",
    "            count_wrong_from_pred = preds_length - count_correct\n",
    "            dict_count[cur_extend_ent].append((count_correct, count_wrong_from_gold, count_wrong_from_pred))\n",
    "\n",
    "    matrix = [v for _,v in dict_count.items()]\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T10:52:28.422182Z",
     "start_time": "2023-12-11T10:52:28.403730Z"
    }
   },
   "outputs": [],
   "source": [
    "def word_level(ent_extend_map, matrix):\n",
    "    new_ent_extend_map = {k:[] for k in ent_extend_map.keys()}\n",
    "    for k in new_ent_extend_map.keys():\n",
    "        # new_ent_extend_map[k] = [k] + ent_extend_map[k]\n",
    "        new_ent_extend_map[k] = ent_extend_map[k]\n",
    "    value = [c for v in new_ent_extend_map.values() for c in v]\n",
    "    word_count_dict = {k:[] for k in value}\n",
    "\n",
    "    tmp_list = []\n",
    "\n",
    "    for idx, row in enumerate(matrix):\n",
    "        correct_sum = 0\n",
    "        wrong_from_gold_sum = 0\n",
    "        wrong_from_pred_sum = 0\n",
    "        for tuple in row:\n",
    "            correct_sum += tuple[0]\n",
    "            wrong_from_gold_sum += tuple[1]\n",
    "            wrong_from_pred_sum += tuple[2]\n",
    "        if correct_sum + wrong_from_pred_sum == 0:\n",
    "            P = 0.0\n",
    "        else:\n",
    "            P = correct_sum / (correct_sum + wrong_from_pred_sum)\n",
    "        if correct_sum + wrong_from_gold_sum == 0:\n",
    "            R = 0.0\n",
    "        else:\n",
    "            R = correct_sum / (correct_sum + wrong_from_gold_sum)\n",
    "        \n",
    "        if P + R == 0:\n",
    "            F1 = 0.0\n",
    "        else:\n",
    "            F1 = 2 * P * R / (P + R)\n",
    "        P_1 = 1 - P\n",
    "        R_1 = 1 - R\n",
    "        tmp_list.append((P, R, P_1, R_1, F1, correct_sum, wrong_from_gold_sum, wrong_from_pred_sum))\n",
    "\n",
    "\n",
    "    for idx,k in enumerate(word_count_dict.keys()):\n",
    "        word_count_dict[k] = tmp_list[idx]\n",
    "\n",
    "    return word_count_dict\n",
    "\n",
    "\n",
    "def word_dict2execl(word_count_dict, ent_extend_map, human_sim, mode):\n",
    "    new_ent_extend_map = {k:[] for k in ent_extend_map.keys()}\n",
    "    for k in new_ent_extend_map.keys():\n",
    "        # new_ent_extend_map[k] = [k] + ent_extend_map[k]\n",
    "        new_ent_extend_map[k] = ent_extend_map[k]\n",
    "\n",
    "    df = pd.DataFrame.from_dict(word_count_dict, orient='index').reset_index()\n",
    "    df.columns = ['扩展实体词','P','R','1-P','1-R','F1','正确个数','golds中错误个数','preds中错误个数']\n",
    "    extend_ent_list = df['扩展实体词'].tolist()\n",
    "\n",
    "    gold_ent_list = [get_type(new_ent_extend_map, ent) for ent in extend_ent_list]\n",
    "    df['实体类型'] = gold_ent_list\n",
    "\n",
    "    cols = list(df.columns)\n",
    "    cols.insert(0, cols.pop(cols.index('实体类型')))\n",
    "    df = df.loc[:, cols]\n",
    "    df.insert(loc=2, column='是否语义相关', value=human_sim)\n",
    "\n",
    "    df.to_excel(f'./results/word_count_dict_{mode}.xlsx',index=False)\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T10:52:30.133534Z",
     "start_time": "2023-12-11T10:52:30.121254Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def split_sheet(groupby_col, mode):  \n",
    "    df = pd.read_excel(f'./results/word_count_dict_{mode}.xlsx')\n",
    "    df_grouped = df.groupby(groupby_col)\n",
    "    type_list = list(df[groupby_col].unique())\n",
    "\n",
    "    with pd.ExcelWriter(f'./results/word_count_dict_{mode}_中间处理表.xlsx', engine='xlsxwriter') as writer:\n",
    "        for type in type_list:\n",
    "            sub_df = df_grouped.get_group(type).copy().reset_index(drop=True)\n",
    "            if r'/' in type:\n",
    "                type = type.replace(r'/', '')\n",
    "            sub_df.to_excel(writer, sheet_name=type, index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CMeEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/cmeee/input_list.json','r',encoding='utf-8') as f:\n",
    "    input_list = json.load(f)\n",
    "\n",
    "with open('./data/cmeee/ent_extend_map.json','r',encoding='utf-8') as f:\n",
    "    ent_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/cmeee/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "\n",
    "with open('./results/test_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    test = [eval(line) for line in f]\n",
    "\n",
    "entity_type_dict = {\n",
    "    'dru':'药物',\n",
    "    'bod':'身体',\n",
    "    'pro':'医疗程序',\n",
    "    'sym':'临床表现',\n",
    "    'equ':'医疗设备',\n",
    "    'ite':'医学检验项目',\n",
    "    'dep':'科室',\n",
    "    'mic':'微生物类',\n",
    "    'dis':'疾病'\n",
    "}\n",
    "\n",
    "matrix = get_count_matrix(ent_extend_map, entity_type_dict, merged_golds, test, mode='zh')\n",
    "word_count_dict = word_level(ent_extend_map, matrix)\n",
    "df_word = word_dict2execl(word_count_dict, ent_extend_map, mode='zh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACE05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/ace05/input_list.json','r',encoding='utf-8') as f:\n",
    "    input_list = json.load(f)\n",
    "\n",
    "with open('./data/ace05/ent_extend_map.json','r',encoding='utf-8') as f:\n",
    "    ent_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/ace05/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "\n",
    "with open('./results/test_en.txt', 'r', encoding='utf-8') as f:\n",
    "    test = [eval(line) for line in f]\n",
    "\n",
    "\n",
    "matrix = get_count_matrix(ent_extend_map, None, merged_golds, test, mode='en')\n",
    "word_count_dict = word_level(ent_extend_map, matrix)\n",
    "df_word = word_dict2execl(word_count_dict, ent_extend_map, mode='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baichuan2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/cmeee/input_list.json','r',encoding='utf-8') as f:\n",
    "    input_list = json.load(f)\n",
    "\n",
    "with open('./data/cmeee/final_entity_extend_map_baichuan2_13B_zh.json','r',encoding='utf-8') as f:\n",
    "    ent_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/cmeee/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "\n",
    "with open('./results/test_baichuan2_13B_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    test = [eval(line) for line in f]\n",
    "\n",
    "entity_type_dict = {\n",
    "    'dru':'药物',\n",
    "    'bod':'身体',\n",
    "    'pro':'医疗程序',\n",
    "    'sym':'临床表现',\n",
    "    'equ':'医疗设备',\n",
    "    'ite':'医学检验项目',\n",
    "    'dep':'科室',\n",
    "    'mic':'微生物类',\n",
    "    'dis':'疾病'\n",
    "}\n",
    "\n",
    "with open('./data/cmeee/ent_sim_human_baichuan2_13B_zh.json','r',encoding='utf-8') as f:\n",
    "    sim_en = json.load(f)\n",
    "res_dict = {k:{} for k in ent_extend_map.keys()}\n",
    "for key in ent_extend_map.keys():\n",
    "    for value in sim_en[key].keys():\n",
    "        if value in ent_extend_map[key]:\n",
    "            res_dict[key][value] = sim_en[key][value]\n",
    "\n",
    "with open('./data/cmeee/final_ent_sim_human_baichuan2_13B_zh.json','w',encoding='utf-8') as f:\n",
    "    json.dump(res_dict,f,indent=4,ensure_ascii=False)\n",
    "\n",
    "human_sim = []\n",
    "for key in res_dict.keys():\n",
    "    for k,v in res_dict[key].items():\n",
    "        human_sim.append(v)\n",
    "        \n",
    "matrix = get_count_matrix(ent_extend_map, entity_type_dict, merged_golds, test, mode='baichuan2_13B_zh')\n",
    "word_count_dict = word_level(ent_extend_map, matrix)\n",
    "df_word = word_dict2execl(word_count_dict, ent_extend_map, human_sim, 'baichuan2_13B_zh')\n",
    "\n",
    "split_sheet('实体类型','baichuan2_13B_zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T10:52:43.270513Z",
     "start_time": "2023-12-11T10:52:42.965534Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('./data/ace05/input_list.json','r',encoding='utf-8') as f:\n",
    "    input_list = json.load(f)\n",
    "\n",
    "with open('./data/ace05/final_entity_extend_map_baichuan2_13B_en.json','r',encoding='utf-8') as f:\n",
    "    ent_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/ace05/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "\n",
    "with open('./results/test_baichuan2_13B_en.txt', 'r', encoding='utf-8') as f:\n",
    "    test = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/ace05/ent_sim_human_baichuan2_13B_en.json','r',encoding='utf-8') as f:\n",
    "    sim_en = json.load(f)\n",
    "res_dict = {k:{} for k in ent_extend_map.keys()}\n",
    "for key in ent_extend_map.keys():\n",
    "    for value in sim_en[key].keys():\n",
    "        if value in ent_extend_map[key]:\n",
    "            res_dict[key][value] = sim_en[key][value]\n",
    "\n",
    "with open('./data/ace05/final_ent_sim_human_baichuan2_13B_en.json','w',encoding='utf-8') as f:\n",
    "    json.dump(res_dict,f,indent=4,ensure_ascii=False)\n",
    "\n",
    "human_sim = []\n",
    "for key in res_dict.keys():\n",
    "    for k,v in res_dict[key].items():\n",
    "        human_sim.append(v)\n",
    "\n",
    "matrix = get_count_matrix(ent_extend_map, None, merged_golds, test, mode='baichuan2_13B_en')\n",
    "word_count_dict = word_level(ent_extend_map, matrix)\n",
    "df_word = word_dict2execl(word_count_dict, ent_extend_map, human_sim, mode='baichuan2_13B_en')\n",
    "\n",
    "split_sheet('实体类型','baichuan2_13B_en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/cmeee/input_list.json','r',encoding='utf-8') as f:\n",
    "    input_list = json.load(f)\n",
    "\n",
    "with open('./data/cmeee/final_entity_extend_map_alpaca_33B_zh.json','r',encoding='utf-8') as f:\n",
    "    ent_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/cmeee/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "\n",
    "with open('./results/test_alpaca_33B_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    test = [eval(line) for line in f]\n",
    "\n",
    "entity_type_dict = {\n",
    "    'dru':'药物',\n",
    "    'bod':'身体',\n",
    "    'pro':'医疗程序',\n",
    "    'sym':'临床表现',\n",
    "    'equ':'医疗设备',\n",
    "    'ite':'医学检验项目',\n",
    "    'dep':'科室',\n",
    "    'mic':'微生物类',\n",
    "    'dis':'疾病'\n",
    "}\n",
    "\n",
    "with open('./data/cmeee/ent_sim_human_alpaca_33B_zh.json','r',encoding='utf-8') as f:\n",
    "    sim_en = json.load(f)\n",
    "res_dict = {k:{} for k in ent_extend_map.keys()}\n",
    "for key in ent_extend_map.keys():\n",
    "    for value in sim_en[key].keys():\n",
    "        if value in ent_extend_map[key]:\n",
    "            res_dict[key][value] = sim_en[key][value]\n",
    "\n",
    "with open('./data/cmeee/final_ent_sim_human_alpaca_33B_zh.json','w',encoding='utf-8') as f:\n",
    "    json.dump(res_dict,f,indent=4,ensure_ascii=False)\n",
    "\n",
    "human_sim = []\n",
    "for key in res_dict.keys():\n",
    "    for k,v in res_dict[key].items():\n",
    "        human_sim.append(v)\n",
    "        \n",
    "matrix = get_count_matrix(ent_extend_map, entity_type_dict, merged_golds, test, mode='alpaca_33B_zh')\n",
    "word_count_dict = word_level(ent_extend_map, matrix)\n",
    "df_word = word_dict2execl(word_count_dict, ent_extend_map, human_sim, 'alpaca_33B_zh')\n",
    "\n",
    "split_sheet('实体类型','alpaca_33B_zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/ace05/input_list.json','r',encoding='utf-8') as f:\n",
    "    input_list = json.load(f)\n",
    "\n",
    "with open('./data/ace05/final_entity_extend_map_alpaca_33B_en.json','r',encoding='utf-8') as f:\n",
    "    ent_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/ace05/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "\n",
    "with open('./results/test_alpaca_33B_en.txt', 'r', encoding='utf-8') as f:\n",
    "    test = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/ace05/ent_sim_human_alpaca_33B_en.json','r',encoding='utf-8') as f:\n",
    "    sim_en = json.load(f)\n",
    "res_dict = {k:{} for k in ent_extend_map.keys()}\n",
    "for key in ent_extend_map.keys():\n",
    "    for value in sim_en[key].keys():\n",
    "        if value in ent_extend_map[key]:\n",
    "            res_dict[key][value] = sim_en[key][value]\n",
    "\n",
    "with open('./data/ace05/final_ent_sim_human_alpaca_33B_en.json','w',encoding='utf-8') as f:\n",
    "    json.dump(res_dict,f,indent=4,ensure_ascii=False)\n",
    "\n",
    "human_sim = []\n",
    "for key in res_dict.keys():\n",
    "    for k,v in res_dict[key].items():\n",
    "        human_sim.append(v)\n",
    "\n",
    "matrix = get_count_matrix(ent_extend_map, None, merged_golds, test, mode='alpaca_33B_en')\n",
    "word_count_dict = word_level(ent_extend_map, matrix)\n",
    "df_word = word_dict2execl(word_count_dict, ent_extend_map, human_sim, mode='alpaca_33B_en')\n",
    "\n",
    "split_sheet('实体类型','alpaca_33B_en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
