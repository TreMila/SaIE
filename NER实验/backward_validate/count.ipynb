{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type(ent_extend_map, ent):\n",
    "    for key, val in ent_extend_map.items():\n",
    "        for v in val:\n",
    "            if v == ent:\n",
    "                return key\n",
    "\n",
    "\n",
    "def get_count_matrix(ent_extend_map, merged_golds, backward):\n",
    "    value = [c for v in ent_extend_map.values() for c in v]\n",
    "    dict = {k: [] for k in value}\n",
    "    dict_count = {k: [] for k in value}\n",
    "\n",
    "    for item in backward:\n",
    "        ent = item[0][1]\n",
    "        dict[ent].append(item)\n",
    "\n",
    "    for k,v in dict.items():   \n",
    "        cur_extend_ent = k\n",
    "        cur_list = v\n",
    "        for item in cur_list:\n",
    "            count_correct = 0\n",
    "            count_wrong_from_gold = 0\n",
    "            count_wrong_from_pred = 0\n",
    "            cur_gold_ent = get_type(ent_extend_map, cur_extend_ent)\n",
    "            cur_idx = item[0][0]\n",
    "            cur_merged_golds = merged_golds[cur_idx]\n",
    "            cur_golds = [gold for gold in cur_merged_golds if gold[1] == cur_gold_ent]\n",
    "            cur_preds = [pred for pred in item] \n",
    "\n",
    "            preds_length = len(cur_preds)\n",
    "            golds_length = len(cur_golds)\n",
    "\n",
    "            if golds_length != 0:\n",
    "                for pred in cur_preds:\n",
    "                    for cmp_gold in cur_golds:\n",
    "                        if pred[2] == '':\n",
    "                            preds_length -= 1\n",
    "                            break\n",
    "                        elif pred[2] in cmp_gold[2] or cmp_gold[2] in pred[2]:\n",
    "                            count_correct += 1\n",
    "                            break\n",
    "            count_correct = min(count_correct, golds_length)\n",
    "            count_wrong_from_gold = golds_length - count_correct\n",
    "            count_wrong_from_pred = preds_length - count_correct\n",
    "            dict_count[cur_extend_ent].append((count_correct, count_wrong_from_gold, count_wrong_from_pred))\n",
    "\n",
    "    matrix = [v for _,v in dict_count.items()]\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def get_gold_count_matrix(ent_extend_map, merged_golds, backward):\n",
    "    dict = {k: [] for k in ent_extend_map.keys()}\n",
    "    dict_count = {k: [] for k in ent_extend_map.keys()}\n",
    "\n",
    "    for item in backward:\n",
    "        ent = item[0][1]\n",
    "        dict[ent].append(item)\n",
    "\n",
    "    for k,v in dict.items():   \n",
    "        cur_gold_ent = k\n",
    "        cur_list = v\n",
    "        for item in cur_list:\n",
    "            count_correct = 0\n",
    "            count_wrong_from_gold = 0\n",
    "            count_wrong_from_pred = 0\n",
    "\n",
    "            cur_idx = item[0][0]\n",
    "            cur_merged_golds = merged_golds[cur_idx]\n",
    "            cur_golds = [gold for gold in cur_merged_golds if gold[1] == cur_gold_ent]\n",
    "            cur_preds = [pred for pred in item] \n",
    "\n",
    "            preds_length = len(cur_preds)\n",
    "            golds_length = len(cur_golds)\n",
    "\n",
    "            if golds_length != 0:\n",
    "                for pred in cur_preds:\n",
    "                    for cmp_gold in cur_golds:\n",
    "                        if pred[2] == '':\n",
    "                            preds_length -= 1\n",
    "                            break\n",
    "                        elif pred[2] in cmp_gold[2] or cmp_gold[2] in pred[2]:\n",
    "                            count_correct += 1\n",
    "                            break\n",
    "            count_correct = min(count_correct, golds_length)\n",
    "            count_wrong_from_gold = golds_length - count_correct\n",
    "            count_wrong_from_pred = preds_length - count_correct\n",
    "            dict_count[cur_gold_ent].append((count_correct, count_wrong_from_gold, count_wrong_from_pred))\n",
    "\n",
    "    matrix = [v for _,v in dict_count.items()]\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_level(ent_extend_map, matrix):\n",
    "    value = [c for v in ent_extend_map.values() for c in v]\n",
    "    word_count_dict = {k:[] for k in value}\n",
    "\n",
    "    tmp_list = []\n",
    "\n",
    "    for idx, row in enumerate(matrix):\n",
    "        correct_sum = 0\n",
    "        wrong_from_gold_sum = 0\n",
    "        wrong_from_pred_sum = 0\n",
    "        for tuple in row:\n",
    "            correct_sum += tuple[0]\n",
    "            wrong_from_gold_sum += tuple[1]\n",
    "            wrong_from_pred_sum += tuple[2]\n",
    "        if correct_sum + wrong_from_pred_sum == 0:\n",
    "            P = 0.0\n",
    "        else:\n",
    "            P = correct_sum / (correct_sum + wrong_from_pred_sum)\n",
    "        R = correct_sum / (correct_sum + wrong_from_gold_sum)\n",
    "        \n",
    "        if P + R == 0:\n",
    "            F1 = 0.0\n",
    "        else:\n",
    "            F1 = 2 * P * R / (P + R)\n",
    "        P_1 = 1 - P\n",
    "        R_1 = 1 - R\n",
    "        tmp_list.append((P, R, P_1, R_1, F1, correct_sum, wrong_from_gold_sum, wrong_from_pred_sum))\n",
    "\n",
    "\n",
    "    for idx,k in enumerate(word_count_dict.keys()):\n",
    "        word_count_dict[k] = tmp_list[idx]\n",
    "\n",
    "    return word_count_dict\n",
    "\n",
    "\n",
    "def word_level_gold(ent_extend_map, matrix,mode):\n",
    "    word_count_dict = {k:[] for k in ent_extend_map.keys()}\n",
    "\n",
    "    tmp_list = []\n",
    "\n",
    "    for idx, row in enumerate(matrix):\n",
    "        correct_sum = 0\n",
    "        wrong_from_gold_sum = 0\n",
    "        wrong_from_pred_sum = 0\n",
    "        for tuple in row:\n",
    "            correct_sum += tuple[0]\n",
    "            wrong_from_gold_sum += tuple[1]\n",
    "            wrong_from_pred_sum += tuple[2]\n",
    "        if correct_sum + wrong_from_pred_sum == 0:\n",
    "            P = 0.0\n",
    "        else:\n",
    "            P = correct_sum / (correct_sum + wrong_from_pred_sum)\n",
    "        R = correct_sum / (correct_sum + wrong_from_gold_sum)\n",
    "        \n",
    "        if P + R == 0:\n",
    "            F1 = 0.0\n",
    "        else:\n",
    "            F1 = 2 * P * R / (P + R)\n",
    "        P_1 = 1 - P\n",
    "        R_1 = 1 - R\n",
    "        tmp_list.append((P, R, P_1, R_1, F1, correct_sum, wrong_from_gold_sum, wrong_from_pred_sum))\n",
    "\n",
    "\n",
    "    for idx,k in enumerate(word_count_dict.keys()):\n",
    "        word_count_dict[k] = tmp_list[idx]\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(word_count_dict, orient='index').reset_index()\n",
    "    df.columns = ['实体类型','P','R','1-P','1-R','F1','正确个数','golds中错误个数','preds中错误个数']\n",
    "    df.to_excel(f'./results/word_count_dict_gold_{mode}.xlsx',index=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def word_dict2execl(word_count_dict, ent_extend_map, sim, mode, entity_type_dict=None):\n",
    "    df = pd.DataFrame.from_dict(word_count_dict, orient='index').reset_index()\n",
    "    df.columns = ['扩展实体词','P','R','1-P','1-R','F1','正确个数','golds中错误个数','preds中错误个数']\n",
    "    extend_ent_list = df['扩展实体词'].tolist()\n",
    "    if '_zh' in mode:\n",
    "        gold_ent_list = [entity_type_dict[get_type(ent_extend_map,ent)] for ent in extend_ent_list]\n",
    "    elif '_en' in mode:\n",
    "        gold_ent_list = [get_type(ent_extend_map,ent) for ent in extend_ent_list]\n",
    "    df['实体类型'] = gold_ent_list\n",
    "\n",
    " \n",
    "    cols = list(df.columns)\n",
    "    cols.insert(0, cols.pop(cols.index('实体类型')))\n",
    "    df = df.loc[:, cols]\n",
    "\n",
    "\n",
    "    if sim != None:\n",
    "        df.insert(loc=2, column='是否语义相关', value=sim)\n",
    "\n",
    "    df.to_excel(f'./results/word_count_dict_{mode}.xlsx',index=False)\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_level(ent_extend_map, sim, matrix, input_length):\n",
    "    length = [len(value) for value in ent_extend_map.values()]\n",
    "\n",
    "    sentences_dict = {k:[] for k in range(input_length)}\n",
    "\n",
    "    l = length[0]\n",
    "    c = 0\n",
    "    t = -1\n",
    "\n",
    "    for idx,key in enumerate(sentences_dict.keys()):\n",
    "        if idx % 10 == 0:\n",
    "            t += 1\n",
    "            if idx == 0:\n",
    "                last_l = 0\n",
    "            else:\n",
    "                last_l += l\n",
    "            l = length[t]\n",
    "        sim_index = last_l\n",
    "        for row in matrix[last_l:last_l + l]:\n",
    "            try:\n",
    "                tuple = row[idx % 10]\n",
    "            except:\n",
    "                print(idx)\n",
    "                print(row)\n",
    "            sentences_dict[key].append((tuple[0], tuple[1], tuple[2], sim[sim_index]))\n",
    "            c += 1\n",
    "            sim_index += 1\n",
    "            if c == l:\n",
    "                c = 0\n",
    "                break\n",
    "            \n",
    "    return sentences_dict\n",
    "\n",
    "\n",
    "\n",
    "def process_sim(sentences_dict, input_list2id, sim_or_not, input_length):\n",
    "    sentences_count_dict = {k:[] for k in range(input_length)}\n",
    "\n",
    "    sentence_matrix = []\n",
    "    for k,v in sentences_dict.items():\n",
    "        sentence_matrix.append(v)\n",
    "\n",
    "    tmp = []\n",
    "\n",
    "    for idx, row in enumerate(sentence_matrix):\n",
    "        correct_sum = 0\n",
    "        wrong_from_gold_sum = 0\n",
    "        wrong_from_pred_sum = 0\n",
    "        for tuple in row:\n",
    "            if tuple[3] == sim_or_not:\n",
    "                correct_sum += tuple[0]\n",
    "                wrong_from_gold_sum += tuple[1]\n",
    "                wrong_from_pred_sum += tuple[2]\n",
    "        if correct_sum + wrong_from_pred_sum == 0:\n",
    "            P = 0.0\n",
    "        else:\n",
    "            P = correct_sum / (correct_sum + wrong_from_pred_sum)\n",
    "        if correct_sum + wrong_from_gold_sum == 0:\n",
    "            R = 0.0\n",
    "        else:\n",
    "            R = correct_sum / (correct_sum + wrong_from_gold_sum)\n",
    "    \n",
    "        P_1 = 1 - P\n",
    "        R_1 = 1 - R\n",
    "        tmp.append((sim_or_not, P, R, P_1, R_1, correct_sum, wrong_from_gold_sum, wrong_from_pred_sum))\n",
    "\n",
    "    for idx,k in enumerate(sentences_count_dict.keys()):\n",
    "        sentences_count_dict[k] = tmp[idx]\n",
    "    \n",
    "    sentence = input_list2id.copy()\n",
    "    for key in sentence.keys():\n",
    "        sentence[key] = sentences_count_dict[input_list2id[key]]\n",
    "\n",
    "    df = pd.DataFrame.from_dict(sentence, orient='index').reset_index()\n",
    "    df.columns = ['句子','是否语义相似','P','R','1-P','1-R','正确个数','golds中错误个数','preds中错误个数']\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def sentence_dict2execl(input_list, sentences_dict, input_length, mode):\n",
    "    input_list2id = {}\n",
    "    for idx, item in enumerate(input_list):\n",
    "        input_list2id[item] = idx\n",
    "    \n",
    "    df_no = process_sim(sentences_dict, input_list2id, 0, input_length)\n",
    "    df_yes = process_sim(sentences_dict, input_list2id, 1, input_length)  \n",
    "    concat_df = pd.concat([df_no, df_yes], axis=0)\n",
    "    concat_df_sorted = concat_df.sort_index()\n",
    "\n",
    "    group = concat_df_sorted.groupby('句子', sort=False)\n",
    "    merged_df = pd.DataFrame()\n",
    "    for _, group_df in group:\n",
    "        group_df = group_df.sort_values(by=['是否语义相似'], ascending=True)\n",
    "        merged_df = pd.concat([merged_df, group_df], axis=0)\n",
    "        \n",
    "    merged_df.to_excel(f'./results/sentence_count_dict_{mode}.xlsx',index=False)\n",
    "    \n",
    "    return merged_df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def all_level(word_count_dict):\n",
    "    grouped = word_count_dict.groupby('实体类型', sort=False)\n",
    "    merged_df = pd.DataFrame(columns=['实体类型', '是否语义相关', '正确个数', 'golds中错误个数', 'preds中错误个数'])\n",
    "\n",
    "    for ent_type, group_df in grouped:\n",
    "        zero_semantic_df = group_df[group_df['是否语义相关'] == 0]\n",
    "        one_semantic_df = group_df[group_df['是否语义相关'] == 1]\n",
    "\n",
    "\n",
    "        merged_row_0 = {\n",
    "            '实体类型': ent_type,\n",
    "            '是否语义相关': 0,\n",
    "            '正确个数': zero_semantic_df['正确个数'].sum(),\n",
    "            'golds中错误个数': zero_semantic_df['golds中错误个数'].sum(),\n",
    "            'preds中错误个数': zero_semantic_df['preds中错误个数'].sum()\n",
    "        }\n",
    "        merged_row_1 = {\n",
    "            '实体类型': ent_type,\n",
    "            '是否语义相关': 1,\n",
    "            '正确个数': one_semantic_df['正确个数'].sum(),\n",
    "            'golds中错误个数': one_semantic_df['golds中错误个数'].sum(),\n",
    "            'preds中错误个数': one_semantic_df['preds中错误个数'].sum()\n",
    "        }\n",
    "        \n",
    "        merged_df = merged_df._append(merged_row_0, ignore_index=True)\n",
    "        # merged_df = pd.concat([merged_df, merged_row_0], axis=0)\n",
    "        merged_df = merged_df._append(merged_row_1, ignore_index=True)\n",
    "        # merged_df = pd.concat([merged_df, merged_row_1], axis=0)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def calculate_scores(row):\n",
    "    correct = row['正确个数']\n",
    "    gold_errors = row['golds中错误个数']\n",
    "    preds_errors = row['preds中错误个数']\n",
    "    if correct + preds_errors == 0:\n",
    "        P = 0.0\n",
    "    else:\n",
    "        P = correct / (correct + preds_errors)\n",
    "    if correct + gold_errors == 0:\n",
    "        R = 0.0\n",
    "    else:\n",
    "        R = correct / (correct + gold_errors)\n",
    "    \n",
    "    P_1 = 1 - P\n",
    "    R_1 = 1 - R\n",
    "\n",
    "    return pd.Series([P, R, P_1, R_1])\n",
    "\n",
    "\n",
    "def count_dict2execl(merged_df, mode):\n",
    "    merged_df[['P','R','P_1','R_1']] = merged_df.apply(calculate_scores, axis=1)\n",
    "    merged_df.to_excel(f'./results/all_count_dict_{mode}.xlsx',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CMeEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/cmeee/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/cmeee/final_entity_extend_map_zh.json','r',encoding='utf-8') as f:\n",
    "    ent_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/cmeee/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/cmeee/backward_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    backward = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/cmeee/backward_gold_chatgpt_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    backward_gold = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/cmeee/ent_sim_human_zh.json','r', encoding='utf-8') as f:\n",
    "    ent_sim_human = json.load(f)\n",
    "\n",
    "input_length = len(input_list)\n",
    "\n",
    "sim = []\n",
    "for key in ent_sim_human.keys():\n",
    "    for k,v in ent_sim_human[key].items():\n",
    "        sim.append(v)\n",
    "\n",
    "entity_type_dict = {\n",
    "    'dru':'药物',\n",
    "    'bod':'身体',\n",
    "    'pro':'医疗程序',\n",
    "    'sym':'临床表现',\n",
    "    'equ':'医疗设备',\n",
    "    'ite':'医学检验项目',\n",
    "    'dep':'科室',\n",
    "    'mic':'微生物类',\n",
    "    'dis':'疾病'\n",
    "}\n",
    "\n",
    "\n",
    "# matrix = get_count_matrix(ent_extend_map, merged_golds, backward)\n",
    "# word_count_dict = word_level(ent_extend_map, matrix)\n",
    "# df_word = word_dict2execl(word_count_dict, ent_extend_map, sim, entity_type_dict=entity_type_dict, mode='zh')\n",
    "\n",
    "# sentences_dict = sentence_level(ent_extend_map, sim, matrix, input_length)\n",
    "# df_sent = sentence_dict2execl(input_list, sentences_dict, input_length, mode='zh')\n",
    "\n",
    "# all_count_dict = all_level(df_word)\n",
    "# count_dict2execl(all_count_dict, mode='zh')\n",
    "matrix = get_gold_count_matrix(ent_extend_map, merged_golds, backward_gold)\n",
    "df_word_gold= word_level_gold(ent_extend_map, matrix, mode='chatgpt_zh')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACE05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/ace05/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/ace05/final_entity_extend_map_en.json','r',encoding='utf-8') as f:\n",
    "    ent_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/ace05/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/ace05/backward_en.txt', 'r', encoding='utf-8') as f:\n",
    "    backward = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/ace05/backward_gold_chatgpt_en.txt', 'r', encoding='utf-8') as f:\n",
    "    backward_gold = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/ace05/ent_sim_human_en.json','r', encoding='utf-8') as f:\n",
    "    ent_sim_human = json.load(f)\n",
    "\n",
    "input_length = len(input_list)\n",
    "\n",
    "sim = []\n",
    "for key in ent_sim_human.keys():\n",
    "    for k,v in ent_sim_human[key].items():\n",
    "        sim.append(v)\n",
    "\n",
    "\n",
    "# matrix = get_count_matrix(ent_extend_map, merged_golds, backward)\n",
    "# word_count_dict = word_level(ent_extend_map, matrix)\n",
    "# df_word = word_dict2execl(word_count_dict, ent_extend_map, sim, mode='en')\n",
    "\n",
    "# sentences_dict = sentence_level(ent_extend_map, sim, matrix, input_length)\n",
    "# df_sent = sentence_dict2execl(input_list, sentences_dict, input_length, mode='en')\n",
    "\n",
    "# all_count_dict = all_level(df_word)\n",
    "# count_dict2execl(all_count_dict, mode='en')\n",
    "matrix = get_gold_count_matrix(ent_extend_map, merged_golds, backward_gold)\n",
    "df_word_gold= word_level_gold(ent_extend_map, matrix, mode='chatgpt_en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpaca-CMeEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/cmeee/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/cmeee/final_entity_extend_map_alpaca_33B_zh.json','r',encoding='utf-8') as f:\n",
    "    ent_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/cmeee/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/cmeee/backward_alpaca_33B_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    backward = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/cmeee/backward_gold_alpaca_33B_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    backward_gold = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/cmeee/ent_sim_human_alpaca_33B_zh.json','r', encoding='utf-8') as f:\n",
    "    ent_sim_human = json.load(f)\n",
    "\n",
    "input_length = len(input_list)\n",
    "\n",
    "sim = []\n",
    "for key in ent_sim_human.keys():\n",
    "    for k,v in ent_sim_human[key].items():\n",
    "        sim.append(v)\n",
    "\n",
    "entity_type_dict = {\n",
    "    'dru':'药物',\n",
    "    'bod':'身体',\n",
    "    'pro':'医疗程序',\n",
    "    'sym':'临床表现',\n",
    "    'equ':'医疗设备',\n",
    "    'ite':'医学检验项目',\n",
    "    'dep':'科室',\n",
    "    'mic':'微生物类',\n",
    "    'dis':'疾病'\n",
    "}\n",
    "\n",
    "\n",
    "# matrix = get_count_matrix(ent_extend_map, merged_golds, backward)\n",
    "# word_count_dict = word_level(ent_extend_map, matrix)\n",
    "# df_word = word_dict2execl(word_count_dict, ent_extend_map, sim, entity_type_dict=entity_type_dict, mode='alpaca_33B_zh')\n",
    "\n",
    "# sentences_dict = sentence_level(ent_extend_map, sim, matrix, input_length)\n",
    "# df_sent = sentence_dict2execl(input_list, sentences_dict, input_length, mode='alpaca_33B_zh')\n",
    "\n",
    "# all_count_dict = all_level(df_word)\n",
    "# count_dict2execl(all_count_dict, mode='alpaca_33B_zh')\n",
    "\n",
    "matrix = get_gold_count_matrix(ent_extend_map, merged_golds, backward_gold)\n",
    "df_word_gold= word_level_gold(ent_extend_map, matrix, mode='alpaca_33B_zh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpaca-ACE05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/ace05/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/ace05/final_entity_extend_map_alpaca_33B_en.json','r',encoding='utf-8') as f:\n",
    "    ent_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/ace05/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/ace05/backward_alpaca_33B_en.txt', 'r', encoding='utf-8') as f:\n",
    "    backward = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/ace05/backward_gold_alpaca_33B_en.txt', 'r', encoding='utf-8') as f:\n",
    "    backward_gold = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/ace05/ent_sim_human_alpaca_33B_en.json','r', encoding='utf-8') as f:\n",
    "    ent_sim_human = json.load(f)\n",
    "\n",
    "input_length = len(input_list)\n",
    "\n",
    "sim = []\n",
    "for key in ent_sim_human.keys():\n",
    "    for k,v in ent_sim_human[key].items():\n",
    "        sim.append(v)\n",
    "\n",
    "\n",
    "# matrix = get_count_matrix(ent_extend_map, merged_golds, backward)\n",
    "# word_count_dict = word_level(ent_extend_map, matrix)\n",
    "# df_word = word_dict2execl(word_count_dict, ent_extend_map, sim, mode='alpaca_33B_en')\n",
    "\n",
    "# sentences_dict = sentence_level(ent_extend_map, sim, matrix, input_length)\n",
    "# df_sent = sentence_dict2execl(input_list, sentences_dict, input_length, mode='alpaca_33B_en')\n",
    "\n",
    "# all_count_dict = all_level(df_word)\n",
    "# count_dict2execl(all_count_dict, mode='alpaca_33B_en')\n",
    "\n",
    "matrix = get_gold_count_matrix(ent_extend_map, merged_golds, backward_gold)\n",
    "df_word_gold= word_level_gold(ent_extend_map, matrix, mode='alpaca_33B_en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llama2-ACE05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/ace05/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/ace05/final_entity_extend_map_llama2_70B_en.json','r',encoding='utf-8') as f:\n",
    "    ent_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/ace05/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/ace05/backward_llama2_70B_en.txt', 'r', encoding='utf-8') as f:\n",
    "    backward = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/ace05/backward_gold_llama2_70B_en.txt', 'r', encoding='utf-8') as f:\n",
    "    backward_gold = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/ace05/ent_sim_human_llama2_70B_en.json','r', encoding='utf-8') as f:\n",
    "    ent_sim_human = json.load(f)\n",
    "\n",
    "input_length = len(input_list)\n",
    "\n",
    "sim = []\n",
    "for key in ent_sim_human.keys():\n",
    "    for k,v in ent_sim_human[key].items():\n",
    "        sim.append(v)\n",
    "\n",
    "\n",
    "# matrix = get_count_matrix(ent_extend_map, merged_golds, backward)\n",
    "# word_count_dict = word_level(ent_extend_map, matrix)\n",
    "# df_word = word_dict2execl(word_count_dict, ent_extend_map, sim, mode='llama2_70B_en')\n",
    "# # df_word = word_dict2execl(word_count_dict, ent_extend_map, None, mode='llama2_70B_en')\n",
    "\n",
    "# sentences_dict = sentence_level(ent_extend_map, sim, matrix, input_length)\n",
    "# df_sent = sentence_dict2execl(input_list, sentences_dict, input_length, mode='llama2_70B_en')\n",
    "\n",
    "# all_count_dict = all_level(df_word)\n",
    "# count_dict2execl(all_count_dict, mode='llama2_70B_en')\n",
    "\n",
    "matrix = get_gold_count_matrix(ent_extend_map, merged_golds, backward_gold)\n",
    "df_word_gold= word_level_gold(ent_extend_map, matrix, mode='llama2_70B_en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGLM-CMeEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/cmeee/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/cmeee/final_entity_extend_map_chatglm_6B_zh.json','r',encoding='utf-8') as f:\n",
    "    ent_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/cmeee/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/cmeee/backward_chatglm_6B_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    backward = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/cmeee/backward_gold_chatglm_6B_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    backward_gold = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/cmeee/ent_sim_human_chatglm_6B_zh.json','r', encoding='utf-8') as f:\n",
    "    ent_sim_human = json.load(f)\n",
    "\n",
    "input_length = len(input_list)\n",
    "\n",
    "sim = []\n",
    "for key in ent_sim_human.keys():\n",
    "    for k,v in ent_sim_human[key].items():\n",
    "        sim.append(v)\n",
    "\n",
    "entity_type_dict = {\n",
    "    'dru':'药物',\n",
    "    'bod':'身体',\n",
    "    'pro':'医疗程序',\n",
    "    'sym':'临床表现',\n",
    "    'equ':'医疗设备',\n",
    "    'ite':'医学检验项目',\n",
    "    'dep':'科室',\n",
    "    'mic':'微生物类',\n",
    "    'dis':'疾病'\n",
    "}\n",
    "\n",
    "\n",
    "# matrix = get_count_matrix(ent_extend_map, merged_golds, backward)\n",
    "# # for m in matrix:\n",
    "# #     print(m)\n",
    "# word_count_dict = word_level(ent_extend_map, matrix)\n",
    "# df_word = word_dict2execl(word_count_dict, ent_extend_map, sim, entity_type_dict=entity_type_dict, mode='chatglm_6B_zh')\n",
    "# # df_word = word_dict2execl(word_count_dict, ent_extend_map, None, entity_type_dict=entity_type_dict, mode='chatglm_6B_zh')\n",
    "\n",
    "# sentences_dict = sentence_level(ent_extend_map, sim, matrix, input_length)\n",
    "# df_sent = sentence_dict2execl(input_list, sentences_dict, input_length, mode='chatglm_6B_zh')\n",
    "\n",
    "# all_count_dict = all_level(df_word)\n",
    "# count_dict2execl(all_count_dict, mode='chatglm_6B_zh')\n",
    "\n",
    "matrix = get_gold_count_matrix(ent_extend_map, merged_golds, backward_gold)\n",
    "df_word_gold= word_level_gold(ent_extend_map, matrix, mode='chatglm_6B_zh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGLM-ACE05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/ace05/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/ace05/final_entity_extend_map_chatglm_6B_en.json','r',encoding='utf-8') as f:\n",
    "    ent_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/ace05/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/ace05/backward_chatglm_6B_en.txt', 'r', encoding='utf-8') as f:\n",
    "    backward = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/ace05/backward_gold_chatglm_6B_en.txt', 'r', encoding='utf-8') as f:\n",
    "    backward_gold = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/ace05/ent_sim_human_chatglm_6B_en.json','r', encoding='utf-8') as f:\n",
    "    ent_sim_human = json.load(f)\n",
    "\n",
    "input_length = len(input_list)\n",
    "\n",
    "sim = []\n",
    "for key in ent_sim_human.keys():\n",
    "    for k,v in ent_sim_human[key].items():\n",
    "        sim.append(v)\n",
    "\n",
    "\n",
    "# matrix = get_count_matrix(ent_extend_map, merged_golds, backward)\n",
    "# word_count_dict = word_level(ent_extend_map, matrix)\n",
    "# df_word = word_dict2execl(word_count_dict, ent_extend_map, sim, mode='chatglm_6B_en')\n",
    "# # df_word = word_dict2execl(word_count_dict, ent_extend_map, None, mode='llama2_70B_en')\n",
    "\n",
    "# sentences_dict = sentence_level(ent_extend_map, sim, matrix, input_length)\n",
    "# df_sent = sentence_dict2execl(input_list, sentences_dict, input_length, mode='chatglm_6B_en')\n",
    "\n",
    "# all_count_dict = all_level(df_word)\n",
    "# count_dict2execl(all_count_dict, mode='chatglm_6B_en')\n",
    "\n",
    "\n",
    "matrix = get_gold_count_matrix(ent_extend_map, merged_golds, backward_gold)\n",
    "df_word_gold= word_level_gold(ent_extend_map, matrix, mode='chatglm_6B_en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT4-CMeEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/cmeee/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "input_list = input_list[80:90]\n",
    "with open('./data/cmeee/final_entity_extend_map_gpt4_zh.json','r',encoding='utf-8') as f:\n",
    "    ent_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/cmeee/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "merged_golds = merged_golds[80:90]\n",
    "with open('./data/cmeee/backward_gpt4_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    backward = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/cmeee/ent_sim_human_gpt4_zh.json','r', encoding='utf-8') as f:\n",
    "    ent_sim_human = json.load(f)\n",
    "\n",
    "input_length = len(input_list)\n",
    "\n",
    "sim = []\n",
    "for key in ent_sim_human.keys():\n",
    "    for k,v in ent_sim_human[key].items():\n",
    "        sim.append(v)\n",
    "\n",
    "entity_type_dict = {\n",
    "    'dru':'药物',\n",
    "    'bod':'身体',\n",
    "    'pro':'医疗程序',\n",
    "    'sym':'临床表现',\n",
    "    'equ':'医疗设备',\n",
    "    'ite':'医学检验项目',\n",
    "    'dep':'科室',\n",
    "    'mic':'微生物类',\n",
    "    'dis':'疾病'\n",
    "}\n",
    "\n",
    "\n",
    "matrix = get_count_matrix(ent_extend_map, merged_golds, backward)\n",
    "\n",
    "word_count_dict = word_level(ent_extend_map, matrix)\n",
    "df_word = word_dict2execl(word_count_dict, ent_extend_map, sim, entity_type_dict=entity_type_dict, mode='gpt4_zh')\n",
    "# df_word = word_dict2execl(word_count_dict, ent_extend_map, None, entity_type_dict=entity_type_dict, mode='chatglm_6B_zh')\n",
    "\n",
    "sentences_dict = sentence_level(ent_extend_map, sim, matrix, input_length)\n",
    "df_sent = sentence_dict2execl(input_list, sentences_dict, input_length, mode='gpt4_zh')\n",
    "\n",
    "all_count_dict = all_level(df_word)\n",
    "count_dict2execl(all_count_dict, mode='gpt4_zh')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT4-ACE05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/ace05/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "input_list = input_list[10:20]\n",
    "with open('./data/ace05/final_entity_extend_map_gpt4_en.json','r',encoding='utf-8') as f:\n",
    "    ent_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/ace05/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "merged_golds = merged_golds[10:20]\n",
    "with open('./data/ace05/backward_gpt4_en.txt', 'r', encoding='utf-8') as f:\n",
    "    backward = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/ace05/ent_sim_human_gpt4_en.json','r', encoding='utf-8') as f:\n",
    "    ent_sim_human = json.load(f)\n",
    "\n",
    "input_length = len(input_list)\n",
    "\n",
    "sim = []\n",
    "for key in ent_sim_human.keys():\n",
    "    for k,v in ent_sim_human[key].items():\n",
    "        sim.append(v)\n",
    "\n",
    "\n",
    "matrix = get_count_matrix(ent_extend_map, merged_golds, backward)\n",
    "word_count_dict = word_level(ent_extend_map, matrix)\n",
    "df_word = word_dict2execl(word_count_dict, ent_extend_map, sim, mode='gpt4_en')\n",
    "# df_word = word_dict2execl(word_count_dict, ent_extend_map, None, mode='llama2_70B_en')\n",
    "\n",
    "sentences_dict = sentence_level(ent_extend_map, sim, matrix, input_length)\n",
    "df_sent = sentence_dict2execl(input_list, sentences_dict, input_length, mode='gpt4_en')\n",
    "\n",
    "all_count_dict = all_level(df_word)\n",
    "count_dict2execl(all_count_dict, mode='gpt4_en')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baichuan2-CMeEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/cmeee/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/cmeee/final_entity_extend_map_baichuan2_13B_zh.json','r',encoding='utf-8') as f:\n",
    "    ent_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/cmeee/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/cmeee/backward_baichuan2_13B_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    backward = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/cmeee/backward_gold_baichuan2_13B_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    backward_gold = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/cmeee/ent_sim_human_baichuan2_13B_zh.json','r', encoding='utf-8') as f:\n",
    "    ent_sim_human = json.load(f)\n",
    "\n",
    "input_length = len(input_list)\n",
    "\n",
    "sim = []\n",
    "for key in ent_sim_human.keys():\n",
    "    for k,v in ent_sim_human[key].items():\n",
    "        sim.append(v)\n",
    "\n",
    "entity_type_dict = {\n",
    "    'dru':'药物',\n",
    "    'bod':'身体',\n",
    "    'pro':'医疗程序',\n",
    "    'sym':'临床表现',\n",
    "    'equ':'医疗设备',\n",
    "    'ite':'医学检验项目',\n",
    "    'dep':'科室',\n",
    "    'mic':'微生物类',\n",
    "    'dis':'疾病'\n",
    "}\n",
    "\n",
    "\n",
    "matrix = get_count_matrix(ent_extend_map, merged_golds, backward)\n",
    "word_count_dict = word_level(ent_extend_map, matrix)\n",
    "df_word = word_dict2execl(word_count_dict, ent_extend_map, sim, entity_type_dict=entity_type_dict, mode='baichuan2_13B_zh')\n",
    "\n",
    "sentences_dict = sentence_level(ent_extend_map, sim, matrix, input_length)\n",
    "df_sent = sentence_dict2execl(input_list, sentences_dict, input_length, mode='baichuan2_13B_zh')\n",
    "\n",
    "all_count_dict = all_level(df_word)\n",
    "count_dict2execl(all_count_dict, mode='baichuan2_13B_zh')\n",
    "\n",
    "# matrix = get_gold_count_matrix(ent_extend_map, merged_golds, backward_gold)\n",
    "# df_word_gold= word_level_gold(ent_extend_map, matrix, mode='baichuan2_13B_zh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baichuan2-ACE05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/ace05/input_list.txt','r',encoding='utf-8') as f:\n",
    "    input_list = [item.strip() for item in f.readlines()]\n",
    "\n",
    "with open('./data/ace05/final_entity_extend_map_baichuan2_13B_en.json','r',encoding='utf-8') as f:\n",
    "    ent_extend_map = json.load(f)\n",
    "\n",
    "with open('./data/ace05/merged_golds.txt', 'r', encoding='utf-8') as f:\n",
    "    merged_golds = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/ace05/backward_baichuan2_13B_en.txt', 'r', encoding='utf-8') as f:\n",
    "    backward = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/ace05/backward_gold_baichuan2_13B_en.txt', 'r', encoding='utf-8') as f:\n",
    "    backward_gold = [eval(line) for line in f]\n",
    "\n",
    "with open('./data/ace05/ent_sim_human_baichuan2_13B_en.json','r', encoding='utf-8') as f:\n",
    "    ent_sim_human = json.load(f)\n",
    "\n",
    "input_length = len(input_list)\n",
    "\n",
    "sim = []\n",
    "for key in ent_sim_human.keys():\n",
    "    for k,v in ent_sim_human[key].items():\n",
    "        sim.append(v)\n",
    "\n",
    "\n",
    "matrix = get_count_matrix(ent_extend_map, merged_golds, backward)\n",
    "word_count_dict = word_level(ent_extend_map, matrix)\n",
    "df_word = word_dict2execl(word_count_dict, ent_extend_map, sim, mode='baichuan2_13B_en')\n",
    "\n",
    "sentences_dict = sentence_level(ent_extend_map, sim, matrix, input_length)\n",
    "df_sent = sentence_dict2execl(input_list, sentences_dict, input_length, mode='baichuan2_13B_en')\n",
    "\n",
    "all_count_dict = all_level(df_word)\n",
    "count_dict2execl(all_count_dict, mode='baichuan2_13B_en')\n",
    "\n",
    "\n",
    "# matrix = get_gold_count_matrix(ent_extend_map, merged_golds, backward_gold)\n",
    "# df_word_gold= word_level_gold(ent_extend_map, matrix, mode='baichuan2_13B_en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
